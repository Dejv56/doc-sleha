<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!--
* FATE#320599: Pacemaker Remote
* See https://trello.com/c/tO9cNZRv for SP3
-->
<?provo dirname="nfs_quick/"?>

<article version="5.0" xml:lang="en" xml:id="art-sle-ha-pmremote"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:dm="urn:x-suse:ns:docmanager"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>&pcmkremquick;</title>
  <subtitle>&productname; &productnumber;</subtitle>
  <info>
   <productnumber>&productnumber;</productnumber>
   <productname>&productname;</productname>
   <date><?dbtimestamp format="m/d/Y X"?></date>
   <authorgroup>
    <author>
     <personname>
      <firstname>Thomas</firstname>
      <surname>Schraitle</surname>
     </personname>
    </author>
   </authorgroup>
   <abstract>
    <para>
     This document guides you through the setup of a cluster with virtual
     guest nodes, managed by &pace; and &pmremote;.
     <emphasis>Remote</emphasis> in this context does not mean physical
     distance, but <quote>non-membership</quote> of a cluster.
    </para>
   </abstract>
   <xi:include href="copyright_techguides.xml" />
  </info>

 <sect1 xml:id="sec-ha-pmremote-usage">
  <title>Usage Scenario</title>
  <para>
   The procedures in this guide describe the process of setting up a minimal
   cluster with the following characteristics:
  </para>
  <itemizedlist>
   <listitem>
    <para>A cluster node running &productname; 12 GA or higher. In this guide,
     its host name is <systemitem class="domainname">&node1;</systemitem>.</para>
   </listitem>
   <listitem>
    <para>Failover of resources from one node to the other if the active host
     breaks down (active/passive setup).</para>
   </listitem>
   <listitem>
    <para>Virtual guest nodes running &pmremote;. In this guide, one guest
     node is named <systemitem class="domainname">&wsII;</systemitem>.
    </para>
   </listitem>
   <listitem>
    <para>
     &pace; to manage guest nodes and remote nodes.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>

  <sect1 xml:id="sec-ha-pmremote-concept">
   <title>Conceptual Overview and Terminology</title>
   <!-- Add a picture? -->
   <para>
    A regular cluster may contain up to 32 nodes. With the &pmremote; service,
    &ha; clusters can be extended to include additional nodes beyond this
    limit by using guest nodes. Unlike remote nodes, a guest node is a
    resource managed by the cluster. As such, they are not bound to the
    32&nbsp;node limitation of the cluster stack. However, they behave
    as regular cluster nodes.
   </para>
   <para>
    Remote nodes do not need to have the full cluster stack installed, as
    they only run the &pmremote; service. The service acts as a proxy,
    allowing the cluster stack on the “regular” cluster nodes to connect to
    the service (see <xref linkend="tab-ha-pmremote-comparison"/>).
    Thus, the node that runs the &pmremote; service is
    effectively integrated into the cluster as a remote node (see <xref
     linkend="vl-ha-pmremote-terminology"/>).
   </para>
   <variablelist xml:id="vl-ha-pmremote-terminology">
    <title>Terminology</title>
    <varlistentry>
     <term>Cluster Node</term>
     <listitem>
      <para>
       A node that runs the complete cluster stack components along with the complete
       &ha; &corosync; stack. </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Pacemaker Remote (&pmremote;)</term>
     <listitem>
      <para>
       A service daemon that makes it possible to use a node as a &pace; node
       without deploying the full cluster stack. Note that &pmremote; is the
       name of the systemd service. However, the name of the daemon
       is &pmrm_daemon; (with a trailing d after its name).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Remote Node</term>
     <listitem>
      <para>
       A physical machine that runs the &pmremote; daemon.
       A remote node runs a special resource (<systemitem
        >ocf:pacemaker:remote</systemitem>) that
       manages communication with the cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Guest Node</term>
     <listitem>
      <para>
       A virtual machine that runs the &pmremote; daemon.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <table xml:id="tab-ha-pmremote-comparison">
    <title>Comparison of Regular HA Stack with and without &pmremote;</title>
    <tgroup cols="2">
     <thead>
      <row>
       <entry>Regular Cluster Stack</entry>
       <entry>Extended Cluster Stack with &pmremote;</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry><informalfigure>
        <mediaobject>
         <imageobject>
          <imagedata fileref="ha_pmremote-without.svg" width="90%"/>
         </imageobject>
        </mediaobject>
       </informalfigure></entry>
       <entry><informalfigure>
        <mediaobject>
         <imageobject>
          <imagedata fileref="ha_pmremote-with.svg" width="90%"/>
         </imageobject>
        </mediaobject>
       </informalfigure></entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <para>
    For a physical machine that contains several virtual guest nodes, the
    process is as follows:
   </para>
   <orderedlist>
    <listitem>
     <para>On the cluster node, virtual machines are launched by &pace;.</para>
    </listitem>
    <listitem>
     <para>The cluster connects to the &pmremote; service of the virtual
      machines.</para>
    </listitem>
    <listitem>
     <para>The virtual machines are integrated into the cluster by &pmremote;.
     </para>
    </listitem>
   </orderedlist>
   <para>
    A regular cluster node may perform the following tasks:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Run cluster resources.
     </para>
    </listitem>
    <listitem>
     <para>Run all command-line tools, such as <command>crm</command>,
      <command>crm_mon</command>.
     </para>
    </listitem>
    <listitem>
     <para>Execute fencing actions.</para>
    </listitem>
    <listitem>
     <para>
      Count toward cluster quorum.
     </para>
    </listitem>
    <listitem>
     <para>
      Serve as the cluster's designated coordinator (DC).
     </para>
    </listitem>
   </itemizedlist>

   <para>
    Remote nodes and guest nodes can run cluster resources and most command
    line tools. However, they have the following limitations:</para>
   <itemizedlist>
    <listitem>
     <para>
      They cannot execute fencing actions.
     </para>
    </listitem>
    <listitem>
     <para>
      They do not affect quorum.
     </para>
    </listitem>
    <listitem>
     <para>
      They cannot serve as Designated Coordinator (DC).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    It is important to distinguish between several roles that a virtual
    machine can take in the &ha; cluster:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      A virtual machine can run a full cluster stack. In this case, the
      virtual machine is a regular cluster node and is not itself managed
      by the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      A virtual machine can be managed by the cluster as a resource, without
      the cluster being aware of the services that run inside the virtual
      machine. In this case, the virtual machine is opaque to the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      A virtual machine can be a cluster resource, and run &pmremote;,
      which allows the cluster to manage services inside the virtual machine.
      In this case, the virtual machine is a guest node and is transparent
      to the cluster.
     </para>
    </listitem>
   </itemizedlist>
  </sect1>

 <sect1 xml:id="sec-ha-pmremote-install">
  <title>Setting Up a Cluster with Virtual Guest Nodes</title>
  <para>In the following example setup, KVM is used for setting up the virtual
   guest nodes:</para>
  <orderedlist>
   <listitem>
    <para><xref linkend="sec-ha-pmremote-config-phys-host"/></para>
   </listitem>
   <listitem>
    <para><xref linkend="sec-ha-pmremote-config-kvm-guest"/></para>
   </listitem>
   <listitem>
    <para><xref linkend="sec-ha-pmremote-integrate-guestsintocluster"/></para>
   </listitem>
  </orderedlist>

  <note>
   <title>Use Static Host Names and IP Addresses</title>
   <para>Add static hosts and IP addresses in <filename>/etc/hosts</filename>.
   </para>
  </note>

  <sect2 xml:id="sec-ha-pmremote-config-phys-host">
   <title>Setting up a Corosync Cluster with Physical Hosts</title>
   <para>
    On the physical host <systemitem class="domainname">&node1;</systemitem>
    proceed as follows:
   </para>
   <procedure>
    <!--<title>Configuring the Physical Host</title>-->
    <step>
     <para>On two physical hosts, set up a basic two-node cluster as
      described in <citetitle>&instquick;</citetitle>.
     <!--<xref linkend="art-ha-install-quick"/>--></para>
    </step>
    <step>
     <para>On &node1;, proceed as follows:</para>
     <substeps>
      <!--
      <step>
       <para>In the firewall settings, open the TCP port 3121 for
        &pmremote;.</para>
      </step>
      -->
      <step>
       <para>Create a specific authentication key for the &pmremote;
        service: </para>
       <screen>&prompt.root;<command>mkdir</command> -p --mode=0755 /etc/pacemaker
&prompt.root;<command>dd</command> if=/dev/urandom of=/etc/pacemaker/authkey bs=4k count=1</screen>
       <para>It needs to be synchronized across all cluster nodes with &csync;.
        The key for the &pmremote; service is different from the cluster
        authentication key that you create in the &yast; cluster module.</para>
      </step>
      <!--
      <step>
       <para>Restart Pacemaker:</para>
       <remark>toms 2017-04-13: Is this really needed?</remark>
       <screen>&prompt.root;<command>systemctl</command> restart pacemaker</screen>
      </step>
      -->
      <step>
       <para>In <filename>/etc/csync2/csync2.cfg</filename>, add the
        following line:</para>
       <screen>include /etc/pacemaker/authkey</screen>
      </step>
     </substeps>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-pmremote-config-kvm-guest">
   <title>Configuring the KVM Guests</title>
   <para>
    The following procedure creates a KVM guest node <systemitem
      class="domainname">&wsII;</systemitem> on your physical host
    <systemitem class="domainname">&node1;</systemitem>:
   </para>
   <procedure>
    <step>
     <para>Create a KVM guest on <systemitem class="domainname"
      >&node1;</systemitem>. For details refer to the
      <citetitle>&virtual;</citetitle>, chapter <citetitle>Guest Installation</citetitle>
      available from <link xlink:href="https://www.suse.com/doc"/>.
     </para>
    </step>
    <step>
     <para>On <systemitem class="domainname"
      >&wsII;</systemitem>, proceed as follows:</para>
     <substeps>
      <step>
       <para>In the firewall settings, open the TCP port 3121 for
        &pmremote;.</para>
      </step>
      <step>
       <para>Install the <package>pacemaker-remote</package> and
         <package>crmsh</package> packages: </para>
       <screen>&prompt.root;<command>zypper</command> in pacemaker-remote crmsh</screen>
      </step>
      <step>
       <para>Enable and start the &pmremote; service on <systemitem
         class="domainname">&node1;</systemitem>:</para>
       <screen>&prompt.root;<command>systemctl</command> enable pacemaker_remote
&prompt.root;<command>systemctl</command> start pacemaker_remote</screen>
      </step>
      <step>
       <para>Create the <filename>/etc/pacemaker</filename> directory and
        change its group:</para>
       <screen>&prompt.root;<command>mkdir</command> -p --mode=0750 /etc/pacemaker
&prompt.root;<command>chgrp</command> haclient /etc/pacemaker</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>On <systemitem class="domainname">&node1;</systemitem>,
      run <command>csync2</command> to copy the authentication key from
      your physical host to your guest node:</para>
     <screen>&prompt.root;<command>csync</command> -xv</screen>
    </step>
    <step>
     <para>
      To verify the host connection to the guest, run <command>ssh</command>
      on one of the cluster nodes. This connection will fail, but
      <emphasis>how</emphasis> it fails gives you hints about its
      connectivity:
     </para>
     <screen>&prompt.root;<command>ssh</command> -p 3121 &wsII;</screen>
     <para>From the output you can tell if your connection works:</para>
     <variablelist>
      <varlistentry>
       <term>Working connection</term>
       <listitem>
        <screen>ssh_exhange_identification: read: Connection reset by peer.</screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Broken connection</term>
       <listitem>
        <screen>ssh: connect to host &wsII; port 3121: No route to host
ssh: connect to host &wsII; port 3121: Connection refused</screen>
        <para>If you see either of the previous messages, the connection does
        not work. Use the <option>-v</option> option for <command>ssh</command>
         and execute the command again to see debugging messages. This could
         be helpful to find connection, authentication, or configuration
         problems. Multiple <option>-v</option> options increase the verbosity.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>If needed, add more guest nodes and configure them as described above.</para>
    </step>
    <step>
     <para>
      Shut down the KVM guest and proceed with <xref
       linkend="sec-ha-pmremote-integrate-guestsintocluster"/>, where &pace;
      will start managing the guest node.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ha-pmremote-integrate-guestsintocluster">
   <title>Integrating KVM Guests into Cluster</title>
   <para>To integrate the KVM guests into the cluster, perform the following
    steps on the physical host <systemitem class="domainname">&node1;</systemitem>:</para>
   <procedure>
    <step>
     <para>Start &pace;:</para>
     <screen>&prompt.root;<command>systemctl</command> start pacemaker</screen>
    </step>
    <step>
     <para>Check if your KVM guest node and IP address is listed
     in the file <filename>/etc/hosts</filename>.</para>
    </step>
    <step xml:id="st-ha-pmremote-integrate-guestsintocluster-virsh-dump">
     <para>Dump the XML configuration of the KVM guest(s) that you need
      in the next step:
     </para>
     <screen>&prompt.root;<command>virsh</command> list --all
 Id    Name         State
-----------------------------------
 -     &wsII;       shut off
&prompt.root;<command>virsh</command> dump &wsII; > /etc/pacemaker/&wsII;.xml</screen>
    </step>
    <step>
     <para>
      Create a <systemitem>VirtualDomain</systemitem> resource to launch the
      virtual machine. Use the dumped configuration from <xref
       linkend="st-ha-pmremote-integrate-guestsintocluster-virsh-dump"/>:
     </para>
     <screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> vm-&wsII; ocf:heartbeat:VirtualDomain \
  params hypervisor="qemu:///system" \
         config="/etc/pacemaker/&wsII;.xml" \
  meta remote-node=&wsII;</screen>
    </step>
    <step>
     <para>Check the status of the cluster with the command
      <command>crm status</command>. It
      should contain a running cluster with nodes that are all accessible.
     </para>
     <remark>toms 2017-04-18: Is there a step missing? Anything like a final
      crm configure primitive venus ocf:pacemaker:remote
     </remark>
     <remark>yangao 2017-04-20: No. ocf:pacemaker:remote resource is only
      needed in the scenario "baremetals as remote nodes".</remark>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-pmremote-testing">
  <title>Testing the Setup</title>
  <para>To demonstrate how resources are executed, use a few dummy resources.
   These dummy resources serve for testing purposes only.
  </para>
  <procedure>
   <step>
    <para>Create a few dummy resources:</para>
    <screen>&prompt.root;<command>crm</command> configure primitive fake1 ocf:pacemaker:Dummy
&prompt.root;<command>crm</command> configure primitive fake2 ocf:pacemaker:Dummy
&prompt.root;<command>crm</command> configure primitive fake3 ocf:pacemaker:Dummy
&prompt.root;<command>crm</command> configure primitive fake4 ocf:pacemaker:Dummy</screen>
   </step>
   <step>
    <para>Check the status of the <command>crm status</command> command.
    You should see something like the following:
    </para>
    <screen>&prompt.root;<command>crm</command> status
[...]
Online: [ &node1; &node2; &wsII; ]

Full list of resources:
fake1           (ocf::pacemaker:Dummy): Started &node1;
fake2           (ocf::pacemaker:Dummy): Started &node1;
fake3           (ocf::pacemaker:Dummy): Started &node1;
fake4           (ocf::pacemaker:Dummy): Started &node1;</screen>
   </step>
   <step>
    <para>To move one of the Dummy primitives to run it on another node
     (&node2;), use the following command:</para>
    <screen>&prompt.root;<command>crm</command> node move fake1 &node2;</screen>
    <remark>yangao 2017-04-20: TODO It makes more sense to move a resource from
     a cluster node to the guest node "venus" and see the effect in here.</remark>
    <para>The status will change to this:</para>
    <screen>&prompt.root;<command>crm</command> status
[...]
Online: [ &node2; &wsII; ]
Offline: [ &node1; ]

Full list of resources:
fake1           (ocf::pacemaker:Dummy): Started &node2;
fake2           (ocf::pacemaker:Dummy): Started &node1;
fake3           (ocf::pacemaker:Dummy): Started &node1;
fake4           (ocf::pacemaker:Dummy): Started &node1;</screen>
   </step>
   <step>
    <para> To test whether fencing works, kill the <systemitem class="daemon"
    >pacemaker_remoted</systemitem> daemon: </para>
    <screen>&prompt.root;<command>kill</command> -9 $(pidof pacemaker_remoted)</screen>
   </step>
   <step>
    <para>After a few seconds, check the status of the cluster again. It
     should look like this:</para>
  <screen>&prompt.root;<command>crm</command> status
[...]
Online: [ &node1; &node2; ]

Full list of resources:
fake1           (ocf::pacemaker:Dummy): Started &node2;
fake2           (ocf::pacemaker:Dummy): Started &node2;
fake3           (ocf::pacemaker:Dummy): Started &node2;
fake4           (ocf::pacemaker:Dummy): Started &node2;</screen>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-pmremote-upgrade">
  <title>Upgrading Cluster and &pmrm; Nodes</title>
  <remark>toms 2017-04-18: Pacemaker version in different SLEHA:
   GA (1.1.12), SP1(1.1.13), SP2(1.1.15)
  </remark>
  <para>
   To upgrade from &sle; 12&nbsp;SP1 to 12&nbsp;SP2, &suse; recommends a
    <quote>rolling upgrade</quote>.
  </para>
  <!-- Taken from ha_migration.xml -->
  <para>
   In a rolling upgrade one cluster node at a time is upgraded while the
   rest of the cluster is still running: you take the first node offline,
   upgrade it and bring it back online to join the cluster. Then you continue
   one by one until all cluster nodes are upgraded to a major version.
   See the section <citetitle>Rolling Upgrade</citetitle> in the &haguide;
   <!--<xref linkend="sec-ha-migration-upgrade-rolling"/>-->
   for more information.
  </para>
  <para>
   To upgrade your cluster and &pmrm; nodes, do the following:
  </para>
  <procedure>
   <step>
    <para>
     Make sure you upgrade your remote nodes to the latest SP1 maintenance update.
    </para>
   </step>
   <step>
    <para>
     Upgrade all cluster nodes to &sle; &productnumber; one by one.
    </para>
   </step>
   <step>
    <para>
     Upgrade all &pmrm; nodes &sle; &productnumber; one by one.
    </para>
   </step>
  </procedure>
  <!--
  <para>
   In the future, rolling upgrades from SP2 to SP3 needs only the last two
   steps.
  </para>
  -->
 </sect1>

 <xi:include href="common_legal.xml"/>
</article>
