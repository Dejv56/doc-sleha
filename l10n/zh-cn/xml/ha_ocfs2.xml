<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="ha_ocfs2.xml" version="5.0" xml:id="cha-ha-ocfs2">
 <title>OCFS2</title>
 <info>
      <abstract>
        <para>
    Oracle Cluster File System 2 (OCFS2) is a general-purpose journaling
    file system that has been fully integrated since the Linux 2.6 Kernel.
    OCFS2 allows you to store application binary files, data files, and
    databases on devices on shared storage. All nodes in a cluster have
    concurrent read and write access to the file system. A user space
    control daemon, managed via a clone resource, provides the integration
    with the HA stack, in particular with Corosync and the Distributed
    Lock Manager (DLM).
   </para>
      </abstract>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>no</dm:translation>
      </dm:docmanager>
    </info>
    <indexterm class="startofrange" xml:id="idx-filesystems-ocfs2">
 <primary>file systems</primary>
 <secondary>OCFS2</secondary></indexterm>
 <sect1 xml:id="sec-ha-ocfs2-features">
  <title>Features and Benefits</title>

  <para>
   OCFS2 can be used for the following storage solutions for example:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     General applications and workloads.
    </para>
   </listitem>
   <listitem>
    <para>
     Xen image store in a cluster. Xen virtual machines and
     virtual servers can be stored on OCFS2 volumes that are mounted by
     cluster servers. This provides quick and easy portability of Xen
     virtual machines between servers.
    </para>
   </listitem>
   <listitem>
    <para>
     LAMP (Linux, Apache, MySQL, and PHP | Perl |
     Python) stacks.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   As a high-performance, symmetric and parallel cluster file system,
   OCFS2 supports the following functions:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     An application's files are available to all nodes in the cluster. Users
     simply install it once on an OCFS2 volume in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     All nodes can concurrently read and write directly to storage via the
     standard file system interface, enabling easy management of
     applications that run across the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     File access is coordinated through DLM. DLM control is good for most
     cases, but an application's design might limit scalability if it
     contends with the DLM to coordinate file access.
    </para>
   </listitem>
   <listitem>
    <para>
     Storage backup functionality is available on all back-end storage. An
     image of the shared application files can be easily created, which can
     help provide effective disaster recovery.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   OCFS2 also provides the following capabilities:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Metadata caching.
    </para>
   </listitem>
   <listitem>
    <para>
     Metadata journaling.
    </para>
   </listitem>
   <listitem>
    <para>
     Cross-node file data consistency.
    </para>
   </listitem>
   <listitem>
    <para>
     Support for multiple-block sizes up to 4 KB, cluster sizes up to 1 MB,
     for a maximum volume size of 4 PB (Petabyte).
    </para>
   </listitem>
   <listitem>
    <para>
     Support for up to 32 cluster nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     Asynchronous and direct I/O support for database files for improved
     database performance.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>Support for OCFS2</title>
   <para>
    OCFS2 is only supported by SUSE when used with the pcmk (Pacemaker) stack,
    as provided by <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase>. SUSE does not provide support for OCFS2 in
    combination with the o2cb stack.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-utils">
  <title>OCFS2 Packages and Management Utilities</title>

  <para>
   The OCFS2 Kernel module (<literal>ocfs2</literal>) is installed
   automatically in the High Availability Extension on SUSEÂ® Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase>. To use
   OCFS2, make sure the following packages are installed on each node in
   the cluster: <package>ocfs2-tools</package> and
   the matching <package>ocfs2-kmp-*</package>
   packages for your Kernel.
  </para>

  <para>
   The <package>ocfs2-tools</package> package
   provides the following utilities for management of OFS2 volumes. For
   syntax information, see their man pages.
  </para>

  <table>
   <title>OCFS2 Utilities</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        OCFS2 Utility
       </para>
      </entry>
      <entry>
       <para>
        Description
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        debugfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Examines the state of the OCFS file system for debugging.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        fsck.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Checks the file system for errors and optionally repairs errors.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mkfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Creates an OCFS2 file system on a device, usually a partition on
        a shared physical or logical disk.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mounted.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Detects and lists all OCFS2 volumes on a clustered system.
        Detects and lists all nodes on the system that have mounted an
        OCFS2 device or lists all OCFS2 devices.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        tunefs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Changes OCFS2 file system parameters, including the volume
        label, number of node slots, journal size for all node slots, and
        volume size.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-create-service">
  <title>Configuring OCFS2 Services and a STONITH Resource</title>



  <para>
   Before you can create OCFS2 volumes, you must configure the following
   resources as services in the cluster: DLM and a STONITH resource.
  </para>

  <para>
   The following procedure uses the <command>crm</command> shell to
   configure the cluster resources. Alternatively, you can also use
   Hawk2 to configure the resources as described in
   <xref linkend="sec-ha-ocfs2-rsc-hawk2"/>.
  </para>

  <procedure xml:id="pro-ocfs2-stonith">
   <title>Configuring a STONITH Resource</title>
   <note>
    <title>STONITH Device Needed</title>
    <para>
     You need to configure a fencing device. Without a STONITH
     mechanism (like <literal>external/sbd</literal>) in place the
     configuration will fail.
    </para>
   </note>
   <step>
    <para>
     Start a shell and log in as <systemitem class="username">root</systemitem> or equivalent.
    </para>
   </step>
   <step>
    <para>
     Create an SBD partition as described in
     <xref linkend="pro-ha-storage-protect-sbd-create"/>.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Configure <literal>external/sbd</literal> as fencing device with
     <literal>/dev/sdb2</literal> being a dedicated partition on the shared
     storage for heartbeating and fencing:
    </para>
<screen><prompt role="custom">crm(live)configure# </prompt><command>primitive</command> sbd_stonith stonith:external/sbd \
  params pcmk_delay_max=30 meta target-role="Started"</screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
   </step>
  </procedure>

  <para>
    For details on configuring the resource group for DLM, see <xref linkend="pro-dlm-resources"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-create">
  <title>Creating OCFS2 Volumes</title>

  <para>
   After you have configured a DLM cluster resource as described in
   <xref linkend="sec-ha-ocfs2-create-service"/>, configure your system to
   use OCFS2 and create OCFs2 volumes.
  </para>

  <note>
   <title>OCFS2 Volumes for Application and Data Files</title>
   <para>
    We recommend that you generally store application files and data files
    on different OCFS2 volumes. If your application volumes and data
    volumes have different requirements for mounting, it is mandatory to
    store them on different volumes.
   </para>
  </note>

  <para>
   Before you begin, prepare the block devices you plan to use for your
   OCFS2 volumes. Leave the devices as free space.
  </para>

  <para>
   Then create and format the OCFS2 volume with the
   <command>mkfs.ocfs2</command> as described in
   <xref linkend="pro-ocfs2-volume"/>. The most important parameters for the
   command are listed in <xref linkend="tab-ha-ofcs2-mkfs-ocfs2-params"/>.
   For more information and the command syntax, refer to the
   <command>mkfs.ocfs2</command> man page.
  </para>

  <table xml:id="tab-ha-ofcs2-mkfs-ocfs2-params">
   <title>Important OCFS2 Parameters</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        OCFS2 Parameter
       </para>
      </entry>
      <entry>
       <para>
        Description and Recommendation
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Volume Label (<option>-L</option>)
       </para>
      </entry>
      <entry>
       <para>
        A descriptive name for the volume to make it uniquely identifiable
        when it is mounted on different nodes. Use the
        <command>tunefs.ocfs2</command> utility to modify the label as
        needed.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Cluster Size (<option>-C</option>)
       </para>
      </entry>
      <entry>
       <para>
        Cluster size is the smallest unit of space allocated to a file to
        hold the data. For the available options and recommendations, refer
        to the <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Number of Node Slots (<option>-N</option>)
       </para>
      </entry>
      <entry>
       <para>
        The maximum number of nodes that can concurrently mount a volume.
        For each of the nodes, OCFS2 creates separate system files, such
        as the journals. Nodes that access the volume
        can be a combination of little-endian architectures (such as AMD64/Intel 64)
        and big-endian architectures (such as S/390x).
       </para>
       <para>
        Node-specific files are called local files. A node slot
        number is appended to the local file. For example:
        <literal>journal:0000</literal> belongs to whatever node is assigned
        to slot number <literal>0</literal>.
       </para>
       <para>
        Set each volume's maximum number of node slots when you create it,
        according to how many nodes that you expect to concurrently mount
        the volume. Use the <command>tunefs.ocfs2</command> utility to
        increase the number of node slots as needed. Note that the value
        cannot be decreased.
       </para>
       <para>
        In case the <option>-N</option> parameter is not specified, the
        number of slots is decided based on the size of the file system.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Block Size (<option>-b</option>)
       </para>
      </entry>
      <entry>
       <para>
        The smallest unit of space addressable by the file system. Specify
        the block size when you create the volume. For the available options
        and recommendations, refer to the <command>mkfs.ocfs2</command> man
        page.

       </para>
      </entry>
     </row>

     <row>
      <entry>
       <para>
        Specific Features On/Off (<option>--fs-features</option>)
       </para>
      </entry>
      <entry>
       <para>
        A comma separated list of feature flags can be provided, and
        <systemitem>mkfs.ocfs2</systemitem> will try to create the file
        system with those features set according to the list. To turn a
        feature on, include it in the list. To turn a feature off, prepend
        <literal>no</literal> to the name.
       </para>
       <para>
        For an overview of all available flags, refer to the
        <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Pre-Defined Features (<option>--fs-feature-level</option>)
       </para>
      </entry>
      <entry>
       <para>
        Allows you to choose from a set of pre-determined file system
        features. For the available options, refer to the
        <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>



  <para>
   If you do not specify any features when creating and formatting
   the volume with <command>mkfs.ocfs2</command>, the following features are
   enabled by default: <option>backup-super</option>,
   <option>sparse</option>, <option>inline-data</option>,
   <option>unwritten</option>, <option>metaecc</option>,
   <option>indexed-dirs</option>, and <option>xattr</option>.
  </para>

  <procedure xml:id="pro-ocfs2-volume">
   <title>Creating and Formatting an OCFS2 Volume</title>
   <para>
    Execute the following steps only on <emphasis>one</emphasis> of the
    cluster nodes.
   </para>
   <step>
    <para>
     Open a terminal window and log in as <systemitem class="username">root</systemitem>.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Create and format the volume using the <command>mkfs.ocfs2</command>
     utility. For information about the syntax for this command, refer to
     the <command>mkfs.ocfs2</command> man page.
    </para>
    <para>
     For example, to create a new OCFS2 file system on
     <filename>/dev/sdb1</filename> that supports up to 32 cluster nodes,
     enter the following commands:
    </para>
<screen><prompt role="root">root # </prompt> mkfs.ocfs2 -N 32 /dev/sdb1</screen>


   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-mount">
  <title>Mounting OCFS2 Volumes</title>

  <para>
   You can either mount an OCFS2 volume manually or with the cluster
   manager, as described in <xref linkend="pro-ocfs2-mount-cluster"/>.
  </para>

  <procedure xml:id="pro-ocfs2-mount-manual">
   <title>Manually Mounting an OCFS2 Volume</title>
   <step>
    <para>
     Open a terminal window and log in as <systemitem class="username">root</systemitem>.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Mount the volume from the command line, using the
     <command>mount</command> command.
    </para>
   </step>
  </procedure>

  <warning>
   <title>Manually Mounted OCFS2 Devices</title>
   <para>
    If you mount the OCFS2 file system manually for testing purposes,
    make sure to unmount it again before starting to use it by means of
    cluster resources.
   </para>
  </warning>

  <procedure xml:id="pro-ocfs2-mount-cluster">
   <title>Mounting an OCFS2 Volume with the Cluster Resource Manager</title>
   <para>
    To mount an OCFS2 volume with the High Availability software, configure an
    ocfs2 file system resource in the cluster. The following procedure uses
    the <command>crm</command> shell to configure the cluster resources.
    Alternatively, you can also use Hawk2 to configure the resources as
    described in <xref linkend="sec-ha-ocfs2-rsc-hawk2"/>.
   </para>
   <step>
    <para>
     Start a shell and log in as <systemitem class="username">root</systemitem> or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Configure Pacemaker to mount the OCFS2 file system on every node in
     the cluster:
    </para>
<screen><prompt role="custom">crm(live)configure# </prompt><command>primitive</command> ocfs2-1 ocf:heartbeat:Filesystem \
  params device="/dev/sdb1" directory="/mnt/shared" \
  fstype="ocfs2" options="acl" \
  op monitor interval="20" timeout="40" \
  op start timeout="60" op stop timeout="60" \
  meta target-role="Started"</screen>
   </step>
   <step>
    <para>
     Add the <literal>ocfs2-1</literal> primitive
     to the <literal>g-storage</literal> group you created in
     <xref linkend="pro-dlm-resources"/>.
    </para>
<screen><prompt role="custom">crm(live)configure# </prompt><command>modgroup</command> g-storage add ocfs2-1</screen>
    <para>The <command>add</command> subcommand appends the new group
     member by default. Because of the base group's internal colocation and ordering, Pacemaker
     will only start the <systemitem class="resource">ocfs2-1</systemitem>
     resource on nodes that also have a <literal>dlm</literal> resource
     already running.
    </para>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.

    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-rsc-hawk2">
  <title>Configuring OCFS2 Resources With Hawk2</title>

  <para>
   Instead of configuring the DLM and the file system resource for OCFS2
   manually with the crm shell, you can also use the OCFS2 template
   in Hawk2's <guimenu>Setup Wizard</guimenu>.
  </para>

  <important>
   <title>Differences Between Manual Configuration and Hawk2</title>
   <para>
    The OCFS2 template in the <guimenu>Setup Wizard</guimenu> does
    <emphasis>not</emphasis> include the configuration of a STONITH
    resource. If you use the wizard, you still need to create an SBD
    partition on the shared storage and configure a STONITH resource as
    described in <xref linkend="pro-ocfs2-stonith"/>.
   </para>
   <remark>taroth 2014-08-15: todo for next release: unify manual configuration
    and configuration via Hawk (use same resources, same options
    etc.), filed https://bugzilla.novell.com/show_bug.cgi?id=892116 for this</remark>
   <para>
    Using the OCFS2 template in the Hawk2 <guimenu>Setup
    Wizard</guimenu> also leads to a slightly different resource
    configuration than the manual configuration described in
    <xref linkend="pro-dlm-resources"/> and
    <xref linkend="pro-ocfs2-mount-cluster"/>.
   </para>
  </important>

  <procedure xml:id="pro-ha-ocfs2-rsc-hawk">
   <title>Configuring OCFS2 Resources with Hawk2's <guimenu>Wizard</guimenu></title>
   <step>
    <para>
     Log in to Hawk2:
    </para>
    <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
   </step>
   <step>
    <para>
     In the left navigation bar, select <guimenu>Wizard</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Expand the <guimenu>File System</guimenu> category and select
     <literal>OCFS2 File System</literal>.
    </para>
   </step>
   <step>
    <para>
     Follow the instructions on the screen. If you need information about an
     option, click it to display a short help text in Hawk2. After the last
     configuration step, <guimenu>Verify</guimenu> the values you have entered.
    </para>
    <para>
     The wizard displays the configuration snippet that will be applied to
     the CIB and any additional changes, if required.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk2-wizard-ocfs2-verify.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="hawk2-wizard-ocfs2-verify.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Check the proposed changes. If everything is according to your wishes,
     apply the changes.
    </para>
    <para>
     A message on the screen shows if the action has been successful.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-quota">
  <title>Using Quotas on OCFS2 File Systems</title>



  <para>
   To use quotas on an OCFS2 file system, create and mount the files
   system with the appropriate quota features or mount options,
   respectively: <literal>ursquota</literal> (quota for individual users) or
   <literal>grpquota</literal> (quota for groups). These features can also
   be enabled later on an unmounted file system using
   <command>tunefs.ocfs2</command>.
  </para>

  <para>
   When a file system has the appropriate quota feature enabled, it tracks
   in its metadata how much space and files each user (or group) uses. Since
   OCFS2 treats quota information as file system-internal metadata, you
   do not need to run the <command>quotacheck</command>(8) program. All
   functionality is built into fsck.ocfs2 and the file system driver itself.
  </para>

  <para>
   To enable enforcement of limits imposed on each user or group, run
   <command>quotaon</command>(8) like you would do for any other file
   system.
  </para>





  <para>
   For performance reasons each cluster node performs quota accounting
   locally and synchronizes this information with a common central storage
   once per 10 seconds. This interval is tuneable with
   <command>tunefs.ocfs2</command>, options
   <option>usrquota-sync-interval</option> and
   <option>grpquota-sync-interval</option>. Therefore quota information may
   not be exact at all times and as a consequence users or groups can
   slightly exceed their quota limit when operating on several cluster nodes
   in parallel.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-ocfs2-more">
  <title>For More Information</title>

  <para>
   For more information about OCFS2, see the following links:
  </para>

  <variablelist>
   <varlistentry>
    <term><link xlink:href="https://ocfs2.wiki.kernel.org/"/>
    </term>
    <listitem>
     <para>
      The OCFS2 project home page.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="http://oss.oracle.com/projects/ocfs2/"/>
    </term>
    <listitem>
     <para>
      The former OCFS2 project home page at Oracle.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="http://oss.oracle.com/projects/ocfs2/documentation"/>
    </term>
    <listitem>
     <para>
      The project's former documentation home page.
     </para>
    </listitem>
   </varlistentry>
  </variablelist><indexterm class="endofrange" startref="idx-filesystems-ocfs2"/>
 </sect1>
</chapter>
