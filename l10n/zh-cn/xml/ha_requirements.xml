<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:dm="urn:x-suse:ns:docmanager" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="ha_requirements.xml" version="5.0" xml:id="cha-ha-requirements">
  <?dbfo-need height="10em"?>


  <title>System Requirements and Recommendations</title>
 <info>
      <dm:docmanager>
        <dm:bugtracker>
          <dm:assignee>taroth@suse.com</dm:assignee>
        </dm:bugtracker>
        <dm:translation>no</dm:translation>
      </dm:docmanager>
      <abstract>
        <para>
    The following section informs you about system requirements, and some
    prerequisites for <phrase role="roductnamereg"><phrase os="sles">SUSE® Linux Enterprise High Availability Extension</phrase></phrase>. It also includes recommendations
    for cluster setup.
   </para>
      </abstract>
    </info>
    <sect1 xml:id="sec-ha-requirements-hw">
  <title>Hardware Requirements</title>

  <para>
   The following list specifies hardware requirements for a cluster based on
   <phrase role="roductnamereg"><phrase os="sles">SUSE® Linux Enterprise High Availability Extension</phrase></phrase>. These requirements represent the minimum hardware
   configuration. Additional hardware might be necessary, depending on how
   you intend to use your cluster.
  </para>

  <variablelist>
   <varlistentry>
    <term>Servers</term>
    <listitem>
     <para> 1 to 32 Linux servers with software as specified in <xref linkend="sec-ha-requirements-sw"/>. </para>
     <para>
      The servers can be bare metal or virtual machines. They do not require
      identical hardware (memory, disk space, etc.), but they must have the
      same architecture. Cross-platform clusters are not supported.
     </para>
     <para> Using <literal>pacemaker_remote</literal>, the cluster can be
      extended to include additional Linux servers beyond the 32-node limit.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Communication Channels</term>
    <listitem>
       <para>
       At least two TCP/IP communication media per cluster node.
       The network equipment must support the communication means you want to use
       for cluster communication: multicast or unicast. The communication
       media should support a data rate of 100 Mbit/s or higher.
       For a supported cluster setup two or more redundant communication paths
       are required. This can be done via:</para>
       <itemizedlist>
        <listitem>
         <para>
          Network Device Bonding (preferred).
         </para>
        </listitem>
        <listitem>
         <para>
          A second communication channel in Corosync.
         </para>
        </listitem>
       </itemizedlist>
     <para>For details, refer to <xref linkend="cha-ha-netbonding"/> and <xref linkend="pro-ha-installation-setup-channel2"/>, respectively.
      
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Node Fencing/STONITH</term>
    <listitem>
      <para>
      To avoid a <quote>split brain</quote> scenario,
      clusters need a node fencing mechanism. In a split brain scenario, cluster
      nodes are divided into two or more groups that do not know about each other
      (because of a hardware or software failure or because of a cut network
      connection). A fencing mechanism isolates the node in question
      (usually by resetting or powering off the node). This is also called
      STONITH (<quote>Shoot the other node in the head</quote>). A node fencing
      mechanism can be either a physical device (a power  switch) or a mechanism
      like SBD (STONITH by disk) in combination with a watchdog. Using SBD
      requires shared storage.
     </para>
     <para>Unless SBD is used, each node in the High Availability cluster must have at least
      one STONITH device. We strongly recommend multiple
      STONITH devices per node.</para>
      <important>
      <title>No Support Without STONITH</title>
      <itemizedlist>
       <listitem>
        <para>You must have a node fencing
        mechanism for your cluster.</para>
       </listitem>
       <listitem>
        <para>The global cluster options
          <systemitem>stonith-enabled</systemitem> and
          <systemitem>startup-fencing</systemitem> must be set to
          <literal>true</literal>.
          When you change them, you lose support.</para>
       </listitem>
      </itemizedlist>
      </important>
    </listitem>
  </varlistentry>
  </variablelist>
 </sect1>
<?dbfo-need height="10em"?>


 <sect1 xml:id="sec-ha-requirements-sw">
  <title>Software Requirements</title>

  <para>
   All nodes that will be part of the cluster need at least the following modules
   and extensions:
  </para>

<itemizedlist>
   <listitem>
    <para>Base System Module <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></para>
   </listitem>
   <listitem>
    <para>Server Applications Module <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></para>
   </listitem>
   <listitem>
    <para><phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></para>
   </listitem>
  </itemizedlist>

   <para>
   Depending on the <systemitem>system roles</systemitem> you select during
   installation, the following software patterns are installed by default:
 </para>
 <table>
  <title>System Roles and Installed Patterns</title>
  <tgroup cols="2">
   <colspec colnum="1" colname="1" colwidth="50*"/>
   <colspec colnum="2" colname="2" colwidth="50*"/>
   <thead>
    <row>
     <entry>
      <para>
       System Role
      </para>
     </entry>
     <entry>
      <para>
       Software Pattern (YaST/Zypper)
      </para>
     </entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry>
      <para>
       HA Node
      </para>
     </entry>
     <entry>
       <itemizedlist>
        <listitem>
         <para>
          High Availability (sles_ha)
         </para>
        </listitem>
        <listitem>
         <para>
         Enhanced Base System (enhanced_base)
         </para>
        </listitem>
       </itemizedlist>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       HA GEO Node
      </para>
     </entry>
      <entry>
       <itemizedlist>
        <listitem>
         <para>
          Geo Clustering for High Availability (ha_geo)
         </para>
        </listitem>
        <listitem>
         <para>
          Enhanced Base System (enhanced_base)
         </para>
        </listitem>
       </itemizedlist>
      </entry>
    </row>
   </tbody>
  </tgroup>
 </table>
  <note>
   <title>Minimal Installation</title>
   <para>
    An installation via those system roles results in a minimal installation only.
    You might need to add more packages manually, if required.</para>
   <para>
    For machines that originally had another system role assigned, you need to
    manually install the <systemitem>sles_ha</systemitem> or
    <systemitem>ha_geo</systemitem> patterns and any further packages that you
    need.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec-ha-requirements-disk">
  <title>Storage Requirements</title>

  <para>
   Some services require shared storage. If using an external NFS share, it must
   be reliably accessible from all cluster nodes via redundant communication
   paths.</para>
  <para>To make data highly available, a shared disk system (Storage Area
   Network, or SAN) is recommended for your cluster. If a shared disk
   subsystem is used, ensure the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The shared disk system is properly set up and functional according to
     the manufacturer’s instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     The disks contained in the shared disk system should be configured to
     use mirroring or RAID to add fault tolerance to the shared disk system.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are using iSCSI for shared disk system access, ensure that you
     have properly configured iSCSI initiators and targets.
    </para>
   </listitem>
   <listitem>
    <para>
     When using DRBD* to implement a mirroring RAID system that distributes
     data across two machines, make sure to only access the device provided
     by DRBD—never the backing device. To leverage the
     redundancy it is possible to use the same NICs as the rest of the cluster.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   When using SBD as STONITH mechanism, additional requirements apply
   for the shared storage. For details, see <xref linkend="sec-ha-storage-protect-req"/>.
   </para>
 </sect1>
 <sect1 xml:id="sec-ha-requirements-other">
  <title>Other Requirements and Recommendations</title>

  <para>
   For a supported and useful High Availability setup, consider the following
   recommendations:
  </para>

  <variablelist>
   <varlistentry xml:id="vle-ha-req-nodes">
    <term>Number of Cluster Nodes</term>
    <listitem>
     <para>For clusters with more than two nodes, it is strongly recommended to use
       an odd number of cluster nodes to have quorum. For more information
       about quorum, see <xref linkend="sec-ha-config-basics-global"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Time Synchronization</term>
     <listitem>
   <para>
     Cluster nodes must synchronize to an NTP server outside the cluster.
     Since <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> 15, chrony is the default implementation of NTP.
     For more information, see the
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-ntp.html">
     <citetitle>Administration Guide</citetitle> for SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></link>.
    </para>
    <para>
     If nodes are not synchronized, the cluster may not work properly.
     In addition, log files and cluster reports are very hard to analyze
     without synchronization.
     If you use the bootstrap scripts, you will be
     warned if NTP is not configured yet.
    </para>
  </listitem>
   </varlistentry>
   <varlistentry>
    <term>Network Interface Card (NIC) Names</term>
    <listitem>
     <para>
      Must be identical on all nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Host Name and IP Address</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        Use static IP addresses.
       </para>
      </listitem>
      <listitem>
        <para>
     List all cluster nodes in the <filename>/etc/hosts</filename> file
     with their fully qualified host name and short host name. It is essential that
     members of the cluster can find each other by name. If the names are not
     available, internal cluster communication will fail.
   </para>
       <para>
        For details on how Pacemaker gets the node names, see also
        <link xlink:href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-node-name.html"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-req-ssh">
    <term>SSH</term>
    <listitem>
      <para>
    All cluster nodes must be able to access each other via SSH. Tools
    like <command>crm report</command> (for troubleshooting) and
    Hawk2's <guimenu>History Explorer</guimenu> require passwordless
    SSH access between the nodes,
    otherwise they can only collect data from the current node.
  </para>
     <note>
      <title>Regulatory Requirements</title>
      <para>
       If passwordless SSH access does not comply with regulatory
       requirements, you can use the work-around described in
       <xref linkend="app-crmreport-nonroot"/> for running 
       <command>crm report</command>.
      </para>
      <para>
       For the <guimenu>History Explorer</guimenu> there is currently no
       alternative for passwordless login.
      </para>
     </note>
    </listitem>
   </varlistentry>

  </variablelist>
 </sect1>
</chapter>
