<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<?provo dirname="install_quick/"?>
<article xmlns="http://docbook.org/ns/docbook" xmlns:dm="urn:x-suse:ns:docmanager" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="art_sle_ha_install_quick.xml" version="5.0" xml:lang="en" xml:id="art-sleha-install-quick">
 <title>Installation and Setup Quick Start</title>
 <subtitle><phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></subtitle>
 <info>
      <productnumber><phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></productnumber>
      <productname><phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase></productname>
      <date>
        <?dbtimestamp ?>


      </date>
      <xi:include href="ha_authors.xml"/>
      <abstract>
        <para>
     This document guides you through the setup of a very basic two-node cluster,
    using the bootstrap scripts provided by the
    <systemitem class="resource">ha-cluster-bootstrap</systemitem> package.
    This includes the configuration of a virtual IP address as a cluster
    resource and the use of SBD on shared storage as a node fencing mechanism.
   </para>
      </abstract>
      <dm:docmanager>
        <dm:translation>no</dm:translation>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec-ha-inst-quick-usage-scenario">
   <title>Usage Scenario</title>
   <para>
    The procedures in this document will lead to a minimal setup of a two-node
    cluster with the following properties:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Two nodes: <systemitem class="server">alice</systemitem> (IP: <systemitem class="ipaddress">192.168.1.1</systemitem>)
      and <systemitem class="server">bob</systemitem> (IP: <systemitem class="ipaddress">192.168.1.2</systemitem>),
      connected to each other via network.
     </para>
    </listitem>
    <listitem>
     <para>
      A floating, virtual IP address (<systemitem class="ipaddress">192.168.2.1</systemitem>) which
      allows clients to connect to the service no matter which physical node it
      is running on.
     </para>
    </listitem>
    <listitem>
     <para>A shared storage device, used as SBD fencing mechanism.
      This avoids split brain scenarios.
     </para>
    </listitem>
    <listitem>
     <para>
      Failover of resources from one node to the other if the active host breaks
      down (<emphasis>active/passive</emphasis> setup).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    After setup of the cluster with the bootstrap scripts, we will monitor
    the cluster with the graphical Hawk2. It is one of the cluster management
    tools included with <phrase role="roductnamereg"><phrase os="sles">SUSEÂ® Linux Enterprise High Availability Extension</phrase></phrase>. As a basic test of whether failover of resources
    works, we will put one of the nodes into standby mode and check if the
    virtual IP address is migrated to the second node.
   </para>
   <para>
    You can use the two-node cluster for testing purposes or as a minimal
    cluster configuration that you can extend later on. Before using the
    cluster in a production environment, modify it according to your
    requirements.
   </para>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-req">
   <title>System Requirements</title>
   <para>
    This section informs you about the key system requirements for the
    scenario described in <xref linkend="sec-ha-inst-quick-usage-scenario" xrefstyle="select:label"/>.
    To adjust the cluster for use in a production environment,
    refer to the full list in <xref linkend="cha-ha-requirements"/>.
   </para>

  <sect2 xml:id="vl-ha-inst-quick-req-hw">
   <title>Hardware Requirements</title>
   <variablelist>
    <varlistentry>
     <term>Servers</term>
     <listitem>
      <para>
       Two servers with software as specified in <xref linkend="il-ha-inst-quick-req-sw"/>.
      </para> <para>
      The servers can be bare metal or virtual machines. They do not require
      identical hardware (memory, disk space, etc.), but they must have the
      same architecture. Cross-platform clusters are not supported.
     </para>
      
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Communication Channels</term>
     <listitem>  <para>
       At least two TCP/IP communication media per cluster node.
       The network equipment must support the communication means you want to use
       for cluster communication: multicast or unicast. The communication
       media should support a data rate of 100 Mbit/s or higher.
       For a supported cluster setup two or more redundant communication paths
       are required. This can be done via:</para>
       <itemizedlist>
        <listitem>
         <para>
          Network Device Bonding (preferred).
         </para>
        </listitem>
        <listitem>
         <para>
          A second communication channel in Corosync.
         </para>
        </listitem>
       </itemizedlist> </listitem>
    </varlistentry>
    <varlistentry>
     <term>Node Fencing/STONITH</term>
     <listitem>  <para>
      To avoid a <quote>split brain</quote> scenario,
      clusters need a node fencing mechanism. In a split brain scenario, cluster
      nodes are divided into two or more groups that do not know about each other
      (because of a hardware or software failure or because of a cut network
      connection). A fencing mechanism isolates the node in question
      (usually by resetting or powering off the node). This is also called
      STONITH (<quote>Shoot the other node in the head</quote>). A node fencing
      mechanism can be either a physical device (a power  switch) or a mechanism
      like SBD (STONITH by disk) in combination with a watchdog. Using SBD
      requires shared storage.
     </para> </listitem>
    </varlistentry>
   </variablelist>
 </sect2>
  <sect2 xml:id="il-ha-inst-quick-req-sw">
   <title>Software Requirements</title>
   <para>
   All nodes that will be part of the cluster need at least the following modules
   and extensions:
  </para>

<itemizedlist>
   <listitem>
    <para>Base System Module <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></para>
   </listitem>
   <listitem>
    <para>Server Applications Module <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></para>
   </listitem>
   <listitem>
    <para><phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></para>
   </listitem>
  </itemizedlist>

  </sect2>
  <sect2 xml:id="vl-ha-inst-quick-req-other">
   <title>Other Requirements and Recommendations</title>
   <variablelist>
    <varlistentry>
     <term>Time Synchronization</term>  <listitem>
   <para>
     Cluster nodes must synchronize to an NTP server outside the cluster.
     Since <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> 15, chrony is the default implementation of NTP.
     For more information, see the
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-ntp.html">
     <citetitle>Administration Guide</citetitle> for SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></link>.
    </para>
    <para>
     If nodes are not synchronized, the cluster may not work properly.
     In addition, log files and cluster reports are very hard to analyze
     without synchronization.
     If you use the bootstrap scripts, you will be
     warned if NTP is not configured yet.
    </para>
  </listitem> </varlistentry>
    <varlistentry>
     <term>Host Name and IP Address</term>
     <listitem>
      <itemizedlist>
       <listitem>
        <para> Use static IP addresses. </para>
       </listitem>
       <listitem>  <para>
     List all cluster nodes in the <filename>/etc/hosts</filename> file
     with their fully qualified host name and short host name. It is essential that
     members of the cluster can find each other by name. If the names are not
     available, internal cluster communication will fail.
   </para> </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>SSH</term>
     <listitem>  <para>
    All cluster nodes must be able to access each other via SSH. Tools
    like <command>crm report</command> (for troubleshooting) and
    Hawk2's <guimenu>History Explorer</guimenu> require passwordless
    SSH access between the nodes,
    otherwise they can only collect data from the current node.
  </para> <para> If you use the bootstrap scripts for setting up the
       cluster, the SSH keys will automatically be created and copied. </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  </sect1>

 <sect1 xml:id="sec-ha-inst-quick-bootstrap">
  <title>Overview of the Bootstrap Scripts</title>
  <para>
   All commands from the <package>ha-cluster-bootstrap</package> package
   execute bootstrap scripts that require only a minimum of time and manual
   intervention.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     With <command>ha-cluster-init</command>, define the basic parameters needed
     for cluster communication. This leaves you with a running one-node cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     With <command>ha-cluster-join</command>, add more nodes to your cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     With <command>ha-cluster-remove</command>, remove nodes from your cluster.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   All bootstrap scripts log to <filename>/var/log/ha-cluster-bootstrap.log</filename>.
   Check this file for any details of the bootstrap process. Any options set
   during the bootstrap process can be modified later with the
   YaST cluster module. See <xref linkend="cha-ha-ycluster"/>
   for details.
  </para>
  <para>Each script comes with a man page covering the range of functions, the
   script's options, and an overview of the files the script can create and modify.
  </para>
  <para>
   The bootstrap script <command>ha-cluster-init</command> checks and
   configures the following components:
  </para>

  <variablelist>
   <varlistentry>
    <term>NTP</term>
    <listitem>
     <para>
      If NTP has not been configured to start at boot time, a message appears.
      Since <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> 15, chrony is the default implementation of NTP.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>It creates SSH keys for passwordless login between cluster nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Csync2</term>
    <listitem>
     <para>
      It configures Csync2 to replicate configuration files across all nodes
      in a cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Corosync</term>
    <listitem>
     <para>It configures the cluster communication system.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SBD/Watchdog</term>
    <listitem>
     <para>It checks if a watchdog exists and asks you whether to configure SBD
      as node fencing mechanism.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Virtual Floating IP</term>
    <listitem>
     <para>It asks you whether to configure a virtual IP address for cluster
      administration with Hawk2.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Firewall</term>
    <listitem>
     <para>It opens the ports in the firewall that are needed for cluster communication.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cluster Name</term>
    <listitem>
     <para>It defines a name for the cluster, by default
        <systemitem>hacluster</systemitem>. This
      is optional and mostly useful for Geo clusters. Usually, the cluster
      name reflects the location and makes it easier to distinguish a site
      inside a Geo cluster.</para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>QDevice/QNetd</term>
    <listitem>
     <para>
      This setup is not covered here. If you want to use a QNetd server,
      you can set it up with the bootstrap script as described in
      <xref linkend="cha-ha-qdevice"/>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

  <sect1 xml:id="sec-ha-inst-quick-installation">
    <title>Installing <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase></title>
    <para>
      The packages for configuring and managing a cluster with the
      High Availability Extension are included in the <literal>High Availability</literal> installation
      pattern (named <literal>sles_ha</literal> on the command line).
      This pattern is only available after <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> has been
      installed as an extension to SUSEÂ® Linux Enterprise Server.
    </para>
    <para>
      For information on how to install
      extensions, see the <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-add-ons.html">
       <citetitle>Deployment Guide</citetitle> for SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></link>.
    </para>

    <procedure xml:id="pro-ha-inst-quick-pattern">
      <title>Installing the <literal>High Availability</literal> Pattern</title>
       <para>
        If the pattern is not installed yet, proceed as follows:
       </para>
      <step>
       <para>
        Install it via command line using Zypper:</para>
<screen><prompt role="root">root # </prompt><command>zypper</command> install -t pattern ha_sles</screen>
      </step>
      <step>
       <para>
          Install the High Availability pattern on <emphasis>all</emphasis> machines that
          will be part of your cluster.
       </para>
       <note>
        <title>Installing Software Packages on All Parties</title>
        <para>
         For an automated installation of SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase> and <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase>
         <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase> use AutoYaST to clone existing nodes. For more information
         see <xref linkend="sec-ha-installation-autoyast"/>.
        </para>
       </note>
      </step>
      <step>
       <para>
         Register the machines at SUSE Customer Center. Find more information in the <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-upgrade-offline.html#sec-update-registersystem">
         <citetitle>Upgrade Guide</citetitle> for SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></link>.
       </para>
      </step>
    </procedure>
  </sect1>

 <sect1 xml:id="sec-ha-inst-quick-sbd">
  <title>Using SBD as Fencing Mechanism</title>
  
   
   <para>
    If you have shared storage, for example, a SAN (Storage Area Network),
    you can use it to avoid split brain scenarios. To do so, configure SBD
    as node fencing mechanism. SBD uses watchdog support
    and the <literal>external/sbd</literal> STONITH resource agent.
   </para>

  <sect2 xml:id="sec-ha-inst-quick-sbd-req">
   <title>Requirements for SBD</title>
   <para>
    During setup of the first node with <command>ha-cluster-init</command>, you
    can decide whether to use SBD. If yes, you need to enter the path to the shared
    storage device. By default, <command>ha-cluster-init</command> will automatically
    create a small partition on the device to be used for SBD.
   </para>
   <para>To use SBD, the following requirements must be met:</para>

   <itemizedlist>
    <listitem>
     <para>The path to the shared storage device must be persistent and
      consistent across all nodes in the cluster. Use stable device names
      such as <filename>/dev/disk/by-id/dm-uuid-part1-mpath-abcedf12345</filename>.
     </para>
    </listitem>
    <listitem>
     <para> The SBD device <emphasis>must not</emphasis>
      use host-based RAID, LVM2, nor reside on a DRBD* instance.
     </para>
    </listitem>
   </itemizedlist>

  <para>
   For details of how to set up shared storage, refer to the
   <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/book-storage.html">
    <citetitle>Storage Administration Guide</citetitle> for SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></link>.
  </para>
  
  </sect2>

  <sect2 xml:id="sec-ha-inst-quick-sbd-setup">
   <title>Enabling the Softdog Watchdog for SBD</title>
   
   <para>
    In SUSE Linux Enterprise Server, watchdog support in the kernel is enabled by default: It ships
    with several kernel modules that provide hardware-specific
    watchdog drivers. The High Availability Extension uses the SBD daemon as the software component
    that <quote>feeds</quote> the watchdog.
   </para>
   <para>
    The following procedure uses the <systemitem>softdog</systemitem>
    watchdog.
   </para>

   
   <important>
    <title>Softdog Limitations</title>
    <para>
     The softdog driver assumes that at least one CPU is still running. If
     all CPUs are stuck, the code in the softdog driver that should reboot the
     system will never be executed. In contrast, hardware watchdogs keep
     working even if all CPUs are stuck.
    </para>
    <para>Before using the cluster in a production environment, we highly
     recommend to replace the <systemitem>softdog</systemitem> module with the
     hardware module that best fits your hardware.
    </para>
    <para>However, if no watchdog matches your hardware,
     <systemitem class="resource">softdog</systemitem> can be used as kernel
     watchdog module.</para>
   </important>

   <procedure xml:id="pro-ha-inst-quick-sbd-setup">
    <step>
     <para>
      Create a persistent, shared storage as described in <xref linkend="sec-ha-inst-quick-sbd-req"/>.
     </para>
    </step>
    <step>
     <para>
      Enable the softdog watchdog:
     </para>
     
     <screen><prompt role="root">root # </prompt><command>echo</command> softdog &gt; /etc/modules-load.d/watchdog.conf
<prompt role="root">root # </prompt><command>systemctl</command> restart systemd-modules-load</screen>
    </step>
    <step>
      
     <para>Test if the softdog module is loaded correctly:
     </para>
     <screen><prompt role="root">root # </prompt><command>lsmod</command> | grep dog
softdog                16384  1</screen>
    </step>
  </procedure>

   <remark>toms 2018-04-05: we need to add a bit more info here how you do
    the tests and what to do when it fails.
    However, this needs some further info from our developers. Some info can
    be found in ha_storage_protection.xml.
    Usually it boils down to "sbd -d DEV list" and "sbd -d DEV message bob test"
   </remark>
   <para>
    We highly recommend to test the SBD fencing mechanism for proper function
    to prevent a split scenario. Such a test can be done by blocking the Corosync
    cluster communication.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec-ha-inst-quick-setup-1st-node">
  <title>Setting Up the First Node</title>
   <para>
   Set up the first node with the <command>ha-cluster-init</command> script.
   This requires only a minimum of time and manual intervention.
  </para>

  <procedure xml:id="pro-ha-inst-quick-setup-ha-cluster-init">
   <title>Setting Up the First Node (<systemitem class="server">alice</systemitem>) with
    <command>ha-cluster-init</command></title>
   <step>
    <para>
     Log in as <systemitem class="username">root</systemitem> to the physical or virtual machine to
     use as cluster node.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing:
    </para>
    <screen><prompt role="root">root # </prompt><command>ha-cluster-init</command> --name <replaceable>CLUSTERNAME</replaceable></screen>
    <para>Replace the <replaceable>CLUSTERNAME</replaceable>
     placeholder with a meaningful name, like the geographical location of your
     cluster (for example, <literal>amsterdam</literal>).
     This is especially helpful to create a Geo cluster later on,
     as it simplifies the identification of a site.
    </para>
    <remark>toms 2016-08-11: the following para relates to
     (a) FATE#320604 allow unicast,
     (b) FATE#320605 identify AWS and use unicast</remark>
    <para>
     If you need unicast instead of multicast (the default) for your cluster
     communication, use the option <option>-u</option>. After installation,
     find the value <literal>udpu</literal> in the file
     <filename>/etc/corosync/corosync.conf</filename>.
     If <command>ha-cluster-init</command> detects a node running on
     Amazon Web Services (AWS), the script will use unicast automatically as
     default for cluster communication.
    </para>
    <para>
     The scripts checks for NTP configuration and a hardware watchdog service.
     It generates the public and private SSH keys used for SSH access and
     Csync2 synchronization and starts the respective services.
    </para>
    
   </step>
   <step>
    <para>
     Configure the cluster communication layer (Corosync):
    </para>
    <substeps>
     <step>
      <para>
       Enter a network address to bind to. By default, the script will
       propose the network address of <systemitem>eth0</systemitem>.
       Alternatively, enter a different network address, for example the
       address of <literal>bond0</literal>.
      </para>
     </step>
     <step>
      <para>
       Enter a multicast address. The script proposes a random address that
       you can use as default. Of course, your particular network needs to
       support this multicast address.
      </para>
     </step>
     <step>
      <para>
       Enter a multicast port. The script proposes <literal>5405</literal>
       as default.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
    Set up SBD as node fencing mechanism:</para>
    <substeps>
     <step>
      <para>Confirm with <literal>y</literal> that you want to use SBD.</para>
     </step>
     <step>
      <para>Enter a persistent path to the partition of your block device that
       you want to use for SBD, see <xref linkend="sec-ha-inst-quick-sbd"/>.
       The path must be consistent across all nodes in the cluster.</para>
     </step>
    </substeps>
   </step>
   <step xml:id="st-ha-cluster-init-ip">
    
    <para>Configure a virtual IP address for cluster administration with
    Hawk2. (We will use this virtual IP resource for testing successful
    failover later on).</para>
    <substeps>
     <step>
      <para>Confirm with <literal>y</literal> that you want to configure a
      virtual IP address.</para></step>
     <step>
      <para>Enter an unused IP address that you want to use as administration IP
       for Hawk2: <literal>192.168.2.1</literal>
      </para>
      <para>Instead of logging in to an individual cluster node with Hawk2,
       you can connect to the virtual IP address.</para>
     </step>
    </substeps>
   </step>
  </procedure>
  <para>
   Finally, the script will start the Pacemaker service to bring the
   cluster online and enable Hawk2. The URL to use for Hawk2 is
   displayed on the screen.
  </para>

  <para>
   You now have a running one-node cluster. To view its status, proceed as follows:
  </para>
  <procedure xml:id="pro-ha-inst-quick-hawk2-login">
   <title>Logging In to the Hawk2 Web Interface</title>
   <step>
    <para> On any machine, start a Web browser and make sure that JavaScript and
     cookies are enabled. </para>
   </step>
   <step>
    <para> As URL, enter the IP address or host name of any cluster node running
     the Hawk Web service. Alternatively, enter the address of the virtual
     IP address that you configured in <xref linkend="st-ha-cluster-init-ip"/>
     of <xref linkend="pro-ha-inst-quick-setup-ha-cluster-init"/>: </para>
    <screen>https://<replaceable>HAWKSERVER</replaceable>:7630/</screen>
    <note>
     <title>Certificate Warning</title>
     <para> If a certificate warning appears when you try to access the URL for
      the first time, a self-signed certificate is in use. Self-signed
      certificates are not considered trustworthy by default. </para>
     <para> Ask your cluster operator for the certificate details to verify the
      certificate. </para>
     <para> To proceed anyway, you can add an exception in the browser to bypass
      the warning. </para>
     
    </note>
   </step>
   <step>
    <para> On the Hawk2 login screen, enter the
      <guimenu>Username</guimenu> and <guimenu>Password</guimenu> of the
       user that has been created during the bootstrap procedure (user <systemitem class="username">hacluster</systemitem>, password
      <literal>linux</literal>).</para>
    <important>
     <title>Secure Password</title>
     <para>Replace the default password with a secure one as soon as possible:
     </para>
     <screen><prompt role="root">root # </prompt><command>passwd</command> hacluster</screen>
    </important>
   </step>
   <step>
    <para>
     Click <guimenu>Log In</guimenu>. After login, the Hawk2 Web interface
     shows the Status screen by default, displaying the current cluster
     status at a glance:
    </para>
    <figure xml:id="fig-ha-inst-quick-one-node-status">
     <title>Status of the One-Node Cluster in Hawk2</title>
     <mediaobject>
      <imageobject>
       <imagedata width="80%" fileref="installquick-one-nodecluster.png"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-ha-inst-quick-setup-2nd-node">
  <title>Adding the Second Node</title>
  <para>
    If you have a one-node cluster up and running, add the second cluster
    node with the <command>ha-cluster-join</command> bootstrap
    script, as described in <xref linkend="pro-ha-inst-quick-setup-ha-cluster-join" xrefstyle="select:label nopage"/>.
    The script only needs access to an existing cluster node and
    will complete the basic setup on the current machine automatically.
    For details, refer to the <command>ha-cluster-join</command> man page.
  </para>
  <para>
   The bootstrap scripts take care of changing the configuration specific to
   a two-node cluster, for example, SBD, Corosync.
  </para>
  <procedure xml:id="pro-ha-inst-quick-setup-ha-cluster-join">
   <title>Adding the Second Node (<systemitem class="server">bob</systemitem>) with
    <command>ha-cluster-join</command></title>
   <step>
    <para>
     Log in as <systemitem class="username">root</systemitem> to the physical or virtual machine supposed to
     join the cluster.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing:
    </para>
<screen><prompt role="root">root # </prompt><command>ha-cluster-join</command></screen>
    <para>
     If NTP has not been configured to start at boot time, a message
     appears. The script also checks for a hardware watchdog device (which
     is important in case you want to configure SBD). You are warned if none
     is present.
    </para>
   </step>
   <step>
    <para>
     If you decide to continue anyway, you will be prompted for the IP
     address of an existing node. Enter the IP address of the first node
     (<systemitem class="server">alice</systemitem>, <systemitem class="ipaddress">192.168.1.1</systemitem>).
    </para>
   </step>
   <step>
    <para>
     If you have not already configured a passwordless SSH access between
     both machines, you will be prompted for the <systemitem class="username">root</systemitem> password
     of the existing node.
    </para>
    <para>
     After logging in to the specified node, the script will copy the
     Corosync configuration, configure SSH, Csync2, and will
     bring the current machine online as new cluster node. Apart from that,
     it will start the service needed for Hawk2. 
    </para>
   </step>
  </procedure>
  <para>
   Check the cluster status in Hawk2. Under <menuchoice>
    <guimenu>Status</guimenu>
    <guimenu>Nodes</guimenu>
   </menuchoice> you should see two nodes with a green status (see
   <xref linkend="fig-ha-inst-quick-two-node-cluster"/>).
  </para>

  <figure xml:id="fig-ha-inst-quick-two-node-cluster">
   <title>Status of the Two-Node Cluster</title>
   <mediaobject>
    <imageobject>
     <imagedata width="80%" fileref="installquick-two-nodecluster-status.png"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>

  <sect1 xml:id="sec-ha-inst-quick-test">
   <title>Testing the Cluster</title>
   <para>
    <xref linkend="sec-ha-inst-quick-test-resource-failover"/> is a simple test to check if the
    cluster moves the virtual IP address to the other node in case the node
    that currently runs the resource is set to <literal>standby</literal>.
   </para>
   <para>However, a realistic test involves specific use cases and scenarios,
    including testing of your fencing mechanism to avoid a split brain
    situation. If you have not set up your fencing mechanism correctly, the cluster
    will not work  properly.</para>
   <para>Before using the cluster in a production environment, test it thoroughly
    according to your use cases or with the help of the <command>ha-cluster-preflight-check</command>
    script.
    
   </para>
   <sect2 xml:id="sec-ha-inst-quick-test-resource-failover">
    <title>Testing Resource Failover</title>
    <para>
     As a quick test, the following procedure checks on resource failovers:
    </para>
   <remark>toms 2016-07-27: Fate#321073
    Tool for Standardize Testing of Basic Cluster Functionality</remark>
   <procedure xml:id="pro-ha-inst-quick-test">
    <title>Testing Resource Failover</title>
    <step>
     <para>
      Open a terminal and ping <systemitem>192.168.2.1</systemitem>,
      your virtual IP address:
     </para>
     <screen><prompt role="root">root # </prompt><command>ping</command> 192.168.2.1</screen>
    </step>
    <step>
     <para>
      Log in to your cluster as described in <xref linkend="pro-ha-inst-quick-hawk2-login"/>.
     </para>
    </step>
    <step>
     <para>
      In Hawk2 <menuchoice>
       <guimenu>Status</guimenu>
       <guimenu>Resources</guimenu>
      </menuchoice>,
      check which node the virtual IP address (resource
      <systemitem>admin_addr</systemitem>) is running on. 
      We assume the resource is running on <systemitem class="server">alice</systemitem>.
      </para>
    </step>
    <step>
     <para>
      Put <systemitem class="server">alice</systemitem> into
      <guimenu>Standby</guimenu> mode (see <xref linkend="fig-ha-inst-quick-standby"/>).
     </para>
     <figure xml:id="fig-ha-inst-quick-standby">
      <title>Node <systemitem class="server">alice</systemitem> in Standby Mode</title>
      <mediaobject>
       <imageobject>
        <imagedata width="60%" fileref="installquick-standby-node.png"/>
       </imageobject>
      </mediaobject>
     </figure>
     
    </step>
    <step>
     <para>
      Click <menuchoice>
       <guimenu>Status</guimenu>
       <guimenu>Resources</guimenu>
      </menuchoice>. The resource <systemitem>admin_addr</systemitem>
      has been migrated to <systemitem class="server">bob</systemitem>.
     </para>
    </step>
   </procedure>
   <para>
    During the migration, you should see an uninterrupted flow of pings to
    the virtual IP address. This shows that the cluster setup and the floating
    IP work correctly. Cancel the <command>ping</command> command with
    <keycombo>
       <keycap function="control"/><keycap>C</keycap>
      </keycombo>.
   </para>
   </sect2>

   <sect2 xml:id="sec-ha-inst-quick-test-with-cluster-script">
    <title>Testing with the ha-cluster-preflight-check Command</title>
    <para>
     The command <command>ha-cluster-preflight-check</command> runs standardized tests
     for a cluster.
     It triggers cluster failures and verifies configuration to find
     problems. Before you use your cluster in production, it is
     recommended to use this command to make sure everything works as expected.
    </para>
    <para>
     The command supports the following checks:
    </para>
    <itemizedlist>
     <listitem>
      <formalpara>
       <title>Environment Check <option>-e</option>/<option>--env-check</option></title>
       <para>
        This test checks:
       </para>
      </formalpara>
      <itemizedlist>
       <listitem>
        <para> Are host names resolvable? </para>
       </listitem>
       <listitem>
        <para>
         Is the time service enabled and started?
        </para>
       </listitem>
       <listitem>
        <para>
         Does the current node have a watchdog configured?
        </para>
       </listitem>
       <listitem>
        <para>
         Is the <command>firewalld</command> service started and are cluster-related ports
         open?
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <formalpara>
       <title>Cluster State Check <option>-c</option>/<option>--cluster-check</option></title>
       <para> Checks different states and services of the cluster. This test checks:</para>
      </formalpara>
      <itemizedlist>
       <listitem>
        <para>
         Are cluster services (Pacemaker/Corosync) enabled and running?
        </para>
       </listitem>
       <listitem>
        <para>
         Is STONITH enabled? It also checks, whether STONITH-related
         resources are configured and started.
         If you configured SBD, is the SBD service started?
        </para>
       </listitem>
       <listitem>
        <para>
         Does the cluster have a quorum?
         Shows current DC nodes and nodes which are online,
         offline, and unclean.
        </para>
       </listitem>
       <listitem>
        <para>
         Do we have started, stopped, or failed resources?
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <formalpara>
       <title>Split Brain Check <option>--split-brain-iptables</option></title>
       <para>
        Simulates a split brain scenario by blocking the Corosync port.
        Checks whether one node can be fenced as expected.
       </para>
      </formalpara>
     </listitem>
     <listitem>
      <formalpara>
       <title>Kills the daemons for SBD, Corosync, and Pacemaker <option>-kill-sbd</option>/<option>-kill-corosync</option>/<option>-kill-pacemakerd</option></title>
       <para>
        After running such a test, you can find a report in
        <filename>/var/lib/ha-cluster-preflight-check</filename>.
        The report includes test case description, action logging,
        and explains possible results.
       </para>
      </formalpara>
     </listitem>
     <listitem>
      <formalpara>
       <title>Fence Node Check <option>--fence-node</option></title>
       <para>Fences the specific node passed from the command line.</para>
      </formalpara>
     </listitem>
    </itemizedlist>
    <para>
     For example, to test the environment, run:
    </para>
    <screen><prompt role="root">root # </prompt><command>crm_mon -1</command>
Stack: corosync
Current DC: alice (version ...) - partition with quorum
Last updated: Fri Mar 03 14:40:21 2020
Last change: Fri Mar 03 14:35:07 2020 by root via cibadmin on alice

2 nodes configured
1 resource configured

Online: [ alice bob ]
Active resources:

 stonith-sbd    (stonith:external/sbd): Started alice

<prompt role="root">root # </prompt><command>ha-cluster-preflight-check</command> -e
[2020/03/20 14:40:45]INFO: Checking hostname resolvable [Pass]
[2020/03/20 14:40:45]INFO: Checking time service [Fail]
 INFO: chronyd.service is available
 WARNING: chronyd.service is disabled
 WARNING: chronyd.service is not active
[2020/03/20 14:40:45]INFO: Checking watchdog [Pass]
[2020/03/20 14:40:45]INFO: Checking firewall [Fail]
 INFO: firewalld.service is available
 WARNING: firewalld.service is not active</screen>
    <para>
     You can inspect the result in <filename>/var/log/ha-cluster-preflight-check.log</filename>.
    </para>
   </sect2>
  </sect1>

  <sect1 xml:id="sec-ha-inst-quick-moreinfo">
   <title>For More Information</title>
    <para>
     More documentation for this product is available at
     <link xlink:href="https://documentation.suse.com/sle-ha/15-SP2"/>.
     For further configuration and administration tasks, see the comprehensive
     <link xlink:href="https://documentation.suse.com/sle-ha/15-SP2/html/SLE-HA-all/book-sleha-guide.html">
      <citetitle>Administration Guide</citetitle></link>.
    </para>
   </sect1>
 <xi:include href="common_copyright_quick.xml"/>
 <xi:include href="common_legal.xml"/>
</article>
