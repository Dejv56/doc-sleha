<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml" 
  title="Profiling step"?>
<?provo dirname="geo_quick/"?>
<article xmlns="http://docbook.org/ns/docbook" xmlns:dm="urn:x-suse:ns:docmanager" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="art_sle_ha_geo_quick.xml" version="5.0" xml:lang="en" xml:id="art-sleha-geo-quick">
 <title>Geo Clustering Quick Start</title>
 <subtitle><phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></subtitle>
 <info>
      <productnumber><phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></productnumber>
      <productname><phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase></productname>
      <date>
        <?dbtimestamp ?>


      </date>
      <xi:include href="ha_authors.xml"/>
      <abstract>
        <para>
     Geo clustering protects workloads across globally distributed data
   centers. This document guides you through the basic setup of a
   Geo cluster, using the Geo bootstrap scripts provided by the
   <systemitem class="resource">ha-cluster-bootstrap</systemitem>
   package.
   </para>
      </abstract>
      <dm:docmanager>
        <dm:translation>no</dm:translation>
      </dm:docmanager>
    </info>
    <sect1 xml:id="sec-ha-geo-quick-concept">
  <title>Conceptual Overview</title>

  <para>
   Geo clusters based on <phrase role="roductnamereg"><phrase os="sles">SUSE® Linux Enterprise High Availability Extension</phrase></phrase> can be considered
   <quote>overlay</quote> clusters where each cluster site corresponds to a
   cluster node in a traditional cluster. The overlay cluster is managed by the
   booth cluster ticket manager (in the following called booth). Each of the
   parties involved in a Geo cluster runs a service, the <systemitem class="daemon">boothd</systemitem>. It
   connects to the booth daemons running on the other sites and exchanges
   connectivity details. For making cluster resources highly available across
   sites, booth relies on cluster objects called tickets. A ticket grants the
   right to run certain resources on a specific cluster site. Booth guarantees
   that every ticket is granted to no more than one site at a time.
  </para>
  <para>
   If the communication between two booth instances breaks down, it might be
   because of a network breakdown between the cluster sites <emphasis>or</emphasis>
   because of an outage of one cluster site. In this case, you need an additional
   instance (a third cluster site or an <literal>arbitrator</literal>) to reach
   consensus about decisions (such as failover of resources across sites).
   Arbitrators are single machines (outside of the clusters) that run a booth
   instance in a special mode. Each Geo cluster can have one or multiple
   arbitrators.
  </para>

  <para>
   <remark>toms 2017-07-04: for the future: maybe also add example IP addresses
    to graphic (node IPs, VIPs for cluster sites)
    </remark>
   <remark>taroth 2017-12-29: FATE#322100 - adjust graphic in case two sites
   *without* arbitrator becomes the default setup? (c#6)
   </remark>
  </para>

  <figure xml:id="fig-ha-geo-quick-example-geosetup">
   <title>Two-Site Cluster—2x2 Nodes + Arbitrator (Optional)</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ha_geocluster.svg" width="100%" format="SVG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ha_geocluster.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   It is also possible to run a two-site Geo cluster <emphasis>without</emphasis>
   an arbitrator. In this case, a Geo cluster administrator needs to manually manage
   the tickets. If a ticket should be granted to more than one site at the same time,
   booth displays a warning.
  </para>

  <para>
   For more details on the concept, components and ticket management used for
   Geo clusters, see the <xref linkend="book-sleha-geo"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-scenario">
  <title>Usage Scenario</title>

  <para>
   In the following, we will set up a basic Geo cluster with two cluster
   sites and one arbitrator:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     We assume the cluster sites are named <literal>amsterdam</literal> and
     <literal>berlin</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     We assume that each site consists of two nodes. The nodes
     <literal>alice</literal> and <literal>bob</literal> belong to the
     cluster <literal>amsterdam</literal>. The nodes
     <literal>charlie</literal> and <literal>doro</literal> belong to the
     cluster <literal>berlin</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     Site <literal>amsterdam</literal> will get the following virtual IP
     address: <literal>192.168.201.100</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     Site <literal>berlin</literal> will get the following virtual IP
     address: <literal>192.168.202.100</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     <remark>taroth 2017-12-29: FATE#322100 - move to Geo Guide? (c#6)</remark>
     We assume that the arbitrator has the following IP address:
     <literal>192.168.203.100</literal>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Before you proceed, make sure the following requirements are fulfilled:
  </para>

  <variablelist>
   <title>Requirements</title>
   <varlistentry>
    <term>Two Existing Clusters</term>
    <listitem>
     <para>
      You have at least two existing clusters that you want to combine into a
      Geo cluster. (If you need to set up two clusters first, follow the
      instructions in the <xref linkend="art-sleha-install-quick"/>.
      </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Meaningful Cluster Names</term>
    <listitem>
     <para>
      Each cluster has a meaningful cluster name defined in <filename>/etc/corosync/corosync.conf</filename>
      that reflects its location.
     </para>
   </listitem>
   </varlistentry>
   <varlistentry>
    <term>Arbitrator</term>
    <listitem>
     <para><remark>taroth 2017-12-29: FATE#322100 - move to Geo Guide? (see
     c#6)</remark>
      You have installed a third machine that is not part of any existing
      clusters and is to be used as arbitrator.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For detailed requirements on each item, see also
   <xref linkend="sec-ha-geo-quick-req"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-req">
  <title>Requirements</title>
   <para>
    All machines (cluster nodes and arbitrators) that will be part of the cluster
    need at least the following modules and extensions:
   </para>

<itemizedlist>
   <listitem>
    <para>Base System Module <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></para>
   </listitem>
   <listitem>
    <para>Server Applications Module <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></para>
   </listitem>
   <listitem>
    <para><phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></para>
   </listitem>
  </itemizedlist>
  <para>
   When installing the machines, select <literal>HA GEO Node</literal> as 
   <systemitem>system role</systemitem>. This leads to the installation of a
   minimal system where the packages from the pattern <literal>Geo Clustering
   for High Availability (ha_geo)</literal> are installed by default.
  </para>
  <itemizedlist>
   <title>Network Requirements</title>
   <listitem>
   <para>
    The virtual IPs to be used for each cluster site must be accessible across
    the Geo cluster.
   </para>
  </listitem>
   <listitem>
   <para>
     The sites must be reachable on one UDP and TCP port per booth instance.
     That means any firewalls or IPsec tunnels in between must be configured
     accordingly.
    </para>
   </listitem>
   <listitem>
    <para>
     Other setup decisions may require to open more ports (for example, for DRBD
     or database replication).
    </para>
   </listitem>
  </itemizedlist>

  <itemizedlist>
  <title>Other Requirements and Recommendations</title>
  <listitem>
   <para>
    All cluster nodes on all sites should synchronize to an NTP server outside
    the cluster. For more information, see the
    <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-ntp.html">
     <citetitle>Administration Guide</citetitle>
     for SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></link>.
   </para>
   <para>
    If nodes are not synchronized, log files and cluster reports are very hard
    to analyze.
   </para>
  </listitem>
  <listitem>
   <para>
    Use an <emphasis>uneven</emphasis> number of sites in your Geo
    cluster. In case the network connection breaks down, this makes sure that
    there still is a majority of sites (to avoid a split brain scenario). In
    case you have an even number of cluster sites, use an arbitrator for
    handling automatic failover of tickets. If you do not use an arbitrator,
    you need to handle ticket failover manually.
   </para>
  </listitem>
  <listitem>
   <para>
    The cluster on each site has a meaningful name, for example:
    <literal>amsterdam</literal> and <literal>berlin</literal>.
   </para>
   <para>
    The cluster names for each site are defined in the respective
    <filename>/etc/corosync/corosync.conf</filename> files:
   </para>
<screen>totem {
    [...]
    cluster_name: amsterdam
    }
</screen>
   <para>
    Change the name with following crmsh command:
   </para>
<screen><prompt role="root">root # </prompt><command>crm</command> cluster rename <replaceable>NEW_NAME</replaceable></screen>
   <para>
    Stop and start the <systemitem class="service">pacemaker</systemitem> service for the changes to take effect:
   </para> <screen><prompt role="root">root # </prompt><command>crm</command> cluster restart</screen>
  </listitem>
 <listitem>
   <para>
     Mixed architectures within one cluster are not supported. However, for
     Geo clusters, each member of the Geo cluster can have a different
     architecture—be it a cluster site or an arbitrator. For example, 
     you can run a Geo cluster with three members (two cluster sites and an
     arbitrator), where one cluster site runs on IBM Z, the other
     cluster site runs on x86, and the arbitrator runs on POWER.
   </para>
 </listitem>
</itemizedlist>

 </sect1>
 <sect1 xml:id="sec-ha-geo-scripts">
  <title>Overview of the Geo Bootstrap Scripts</title>
  <itemizedlist>
   <listitem>
    <para>
     With <command>ha-cluster-geo-init</command>, turn a cluster into the first
     site of a Geo cluster. The script takes some parameters like the names of
     the clusters, the arbitrator, and one or multiple tickets and creates
     <filename>/etc/booth/booth.conf</filename> from them. It copies the booth configuration to all nodes on the
     current cluster site. It also configures the cluster resources needed for
     booth on the current cluster site.
    </para>
    <para>
     For details, see <xref linkend="sec-ha-geo-quick-ha-cluster-geo-init"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     With <command>ha-cluster-geo-join</command>, add the current cluster to an
     existing Geo cluster. The script copies the booth configuration from an
     existing cluster site and writes it to <filename>/etc/booth/booth.conf</filename> on all nodes on the
     current cluster site. It also configures the cluster resources needed for
     booth on the current cluster site.
    </para>
    <para>
     For details, see <xref linkend="sec-ha-geo-quick-ha-cluster-geo-join"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     With <command>ha-cluster-geo-init-arbitrator</command>, turn the current
     machine into an arbitrator for the Geo cluster. The script copies the
     booth configuration from an existing cluster site and writes it to
     <filename>/etc/booth/booth.conf</filename>.
    </para>
    <para>
     For details, see
     <xref linkend="sec-ha-geo-quick-ha-cluster-geo-init-arbitrator"/>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   All bootstrap scripts log to
   <filename>/var/log/ha-cluster-bootstrap.log</filename>. Check the log file
   for any details of the bootstrap process. Any options set during the
   bootstrap process can be modified later (by modifying booth settings,
   modifying resources etc.). For details, see the <xref linkend="book-sleha-geo"/>.
  </para>


 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-inst">
  <title>Installation</title>

  <para>
   Support for using High Availability clusters across unlimited distances is available with
   Geo Clustering for SUSE Linux Enterprise High Availability Extension.
  </para>
  <para>
   For setting up a Geo cluster you need the packages included in the
   following installation patterns:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <literal>High Availability</literal> (named <literal>sles_ha</literal> on the command line)
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>Geo Clustering for High Availability</literal> (named <literal>ha_geo</literal> on the command line)
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Both patterns are only available if you have registered your system at SUSE Customer Center
   (or a local registration server) and have added the respective modules or
   installation media as an extension. For information on how to
   install extensions, see the <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-add-ons.html">
    <citetitle>Deployment Guide</citetitle> for SUSE Linux Enterprise <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase></link>.
  </para>
  <para>
     To install the packages from both patterns via command line, use Zypper:
    </para>
<screen><prompt role="root">root # </prompt><command>zypper</command> install -t pattern ha_sles ha_geo</screen>

  <important>
   <title>Installing Software Packages on all Parties</title>
   <para>
    The software packages needed for High Availability and Geo clusters are
    <emphasis>not</emphasis> automatically copied to the cluster nodes.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Install SUSE Linux Enterprise Server <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase> and the <literal>ha_sles</literal> and
      <literal>ha_geo</literal> patterns on <emphasis>all</emphasis> machines
      that will be part of your Geo cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      Instead of manually installing the packages on all machines that will be
      part of your cluster, use AutoYaST to clone existing nodes. Find more
      information at <xref linkend="sec-ha-installation-autoyast"/>.
     </para>
    </listitem>
   </itemizedlist>
  </important>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-ha-cluster-geo-init">
  <title>Setting Up the First Site of a Geo Cluster</title>

  <para>
   Use the <command>ha-cluster-geo-init</command> script to turn an existing
   cluster into the first site of a Geo cluster.
  </para>

  <procedure xml:id="pro-ha-geo-quick-ha-cluster-geo-init">
   <title>Setting Up the First Site (<systemitem>amsterdam</systemitem>) with <command>ha-cluster-geo-init</command></title>
   <step>
    <para>
     Define a virtual IP per cluster site that can be used to access the site.
     We assume using <literal>192.168.201.100</literal> and
     <literal>192.168.202.100</literal> for this purpose. You do not need to
     configure the virtual IPs as cluster resources yet. This will be done by
     the bootstrap scripts.
    </para>
   </step>
   <step>
    <para>
     Define the name of at least one ticket that will grant the right to run
     certain resources on a cluster site. Use a meaningful name that reflects
     the resources that will depend on the ticket (for example,
     <literal>ticket-nfs</literal>). The bootstrap scripts only need the ticket
     name—you can define the remaining details (ticket dependencies of
     the resources) later on, as described in
     <xref linkend="sec-ha-geo-quick-next"/>.
    </para>
   </step>
   <step>
    <para>
     Log in to a node of an existing cluster (for example, on node
     <literal>alice</literal> of the cluster <literal>amsterdam</literal>).
    </para>
   </step>
   <step xml:id="st-ha-cluster-geo-init">
    <para>
     Run <command>ha-cluster-geo-init</command>. For example, use the following
     options:
     <remark>taroth 2017-12-29: FATE#322100 - remove the --arbitrator option?
     (see c#6)</remark>
    </para>
<screen><prompt role="root">root # </prompt><command>ha-cluster-geo-init</command> \
  --clusters<co xml:id="co-geo-init-clusters"/> "amsterdam=192.168.201.100 berlin=192.168.202.100" \
  --tickets<co xml:id="co-geo-init-ticket"/> ticket-nfs \
  --arbitrator<co xml:id="co-geo-init-arbitrator"/> 192.168.203.100</screen>
    <calloutlist>
     <callout arearefs="co-geo-init-clusters">
       <para>
    The names of the cluster sites (as defined in <filename>/etc/corosync/corosync.conf</filename>) and the virtual
    IP addresses you want to use for each cluster site. In this case, we have
    two cluster sites (<literal>amsterdam</literal> and
    <literal>berlin</literal>) with a virtual IP address each.
   </para>
     </callout>
     <callout arearefs="co-geo-init-ticket">
      <para>
       The name of one or multiple tickets.
      </para>
     </callout>
     <callout arearefs="co-geo-init-arbitrator">
      <para>
       The host name or IP address of a machine outside of the clusters.
      </para>
     </callout>
    </calloutlist>
   </step>
  </procedure>

  <para>
   The bootstrap script creates the booth configuration file and synchronizes
   it across the cluster sites. It also creates the basic cluster resources
   needed for booth. <xref linkend="st-ha-cluster-geo-init" xrefstyle="seletc:label"/> of
   <xref linkend="pro-ha-geo-quick-ha-cluster-geo-init" xrefstyle="select:label"/>
   would result in the following booth configuration and cluster resources:
   <remark>taroth 2017-12-29: FATE#322100 - remove the arbitrator entry? (c#6)</remark>
  </para>

  <example xml:id="ex-geo-init-booth-conf">
   <title>Booth Configuration Created By <command>ha-cluster-geo-init</command></title>
<screen># The booth configuration file is "/etc/booth/booth.conf". You need to
# prepare the same booth configuration file on each arbitrator and
# each node in the cluster sites where the booth daemon can be launched.

# "transport" means which transport layer booth daemon will use.
# Currently only "UDP" is supported.
transport="UDP"
port="9929"

arbitrator="192.168.203.100"
site="192.168.201.100"
site="192.168.202.100"
authfile="/etc/booth/authkey"
ticket="ticket-nfs"
expire="600"</screen>
  </example>

  <example xml:id="ex-geo-init-rsc-conf">
   <title>Cluster Resources Created by <command>ha-cluster-geo-init</command></title>
<screen>primitive<co xml:id="co-geo-quick-rsc-booth-ip"/> booth-ip IPaddr2 \
  params rule #cluster-name eq amsterdam ip=192.168.201.100 \
  params rule #cluster-name eq berlin ip=192.168.202.100 \
primitive<co xml:id="co-geo-quick-rsc-booth-site"/> booth-site ocf:pacemaker:booth-site \
  meta resource-stickiness=INFINITY \
  params config=booth \
  op monitor interval=10s
group<co xml:id="co-geo-quick-rsc-g-booth"/> g-booth booth-ip booth-site \
meta target-role=Stopped<co xml:id="co-geo-quick-rsc-stopped"/></screen>
   <calloutlist>
    <callout arearefs="co-geo-quick-rsc-booth-ip">
     <para>
      A virtual IP address for each cluster site. It is required by the booth
      daemons who need a persistent IP address on each cluster site.
     </para>
    </callout>
    <callout arearefs="co-geo-quick-rsc-booth-site">
     <para>
      A primitive resource for the booth daemon. It communicates with the booth
      daemons on the other cluster sites. The daemon can be started on any
      node of the site. To make the resource stay on the same node, if possible,
      resource-stickiness is set to <literal>INFINITY</literal>.
     </para>
    </callout>
    <callout arearefs="co-geo-quick-rsc-g-booth">
     <para>
      A cluster resource group for both primitives. With this configuration, each
      booth daemon will be available at its individual IP address, independent of
      the node the daemon is running on.
     </para>
    </callout>
    <callout arearefs="co-geo-quick-rsc-stopped">
     <para>
      The cluster resource group is not started by default. After verifying the
      configuration of your cluster resources (and adding the resources you
      need to complete your setup), you need to start the resource group. See
      <xref linkend="vl-ha-geo-quick-next-req"/> for details.
     </para>
    </callout>
   </calloutlist>
  </example>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-ha-cluster-geo-join">
  <title>Adding Another Site to a Geo Cluster</title>

  <para>
   After you have initialized the first site of your Geo cluster, add the
   second cluster with <literal>ha-cluster-geo-join</literal>, as described in
   <xref linkend="pro-ha-geo-quick-ha-cluster-geo-join" xrefstyle="select:label"/>.
   The script needs SSH access to an already configured cluster site and will
   add the current cluster to the Geo cluster.
  </para>

  <procedure xml:id="pro-ha-geo-quick-ha-cluster-geo-join">
   <title>Adding the Second Site (<literal>berlin</literal>) with <command>ha-cluster-geo-join</command></title>
   <step>
    <para>
     Log in to a node of the cluster site you want to add (for example, on node
     <literal>charlie</literal> of the cluster <literal>berlin</literal>).
    </para>
   </step>
   <step xml:id="st-ha-cluster-geo-join">
    <para>
     Run the <command>ha-cluster-geo-join</command> command. For example:
    </para>
<screen><prompt role="root">root # </prompt><command>ha-cluster-geo-join</command> \
  --cluster-node<co xml:id="co-geo-join-cluster-node"/> 192.168.201.100\
  --clusters<co xml:id="co-geo-join-clusters"/> "amsterdam=192.168.201.100 berlin=192.168.202.100"
     </screen>
    <calloutlist>
     <callout arearefs="co-geo-join-cluster-node">
      <para>
       Specifies where to copy the booth configuration from. Use the IP address
       or host name of a node in an already configured Geo cluster site.
       You can also use the virtual IP address of an already existing cluster
       site (like in this example). Alternatively, use the IP address or host
       name of an already configured arbitrator for your Geo cluster.
      </para>
     </callout>
     <callout arearefs="co-geo-join-clusters">
       <para>
    The names of the cluster sites (as defined in <filename>/etc/corosync/corosync.conf</filename>) and the virtual
    IP addresses you want to use for each cluster site. In this case, we have
    two cluster sites (<literal>amsterdam</literal> and
    <literal>berlin</literal>) with a virtual IP address each.
   </para>
     </callout>
    </calloutlist>
   </step>
  </procedure>

  <para>
   The <command>ha-cluster-geo-join</command> script copies the booth
   configuration from
   <xref linkend="co-geo-join-cluster-node" xrefstyle="select:label"/>, see
   <xref linkend="ex-geo-init-booth-conf" xrefstyle="select:label"/>. In
   addition, it creates the cluster resources needed for booth (see
   <xref linkend="ex-geo-init-rsc-conf" xrefstyle="select:label"/>).
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-ha-cluster-geo-init-arbitrator">
  <title>Adding the Arbitrator</title>

  <para>
   <remark>taroth 2017-12-29: FATE#322100 - move this section to Geo Guide? (c#6)</remark>
   After you have set up all sites of your Geo cluster with
   <command>ha-cluster-geo-init</command> and
   <command>ha-cluster-geo-join</command>, set up the arbitrator with
   <command>ha-cluster-geo-init-arbitrator</command>.
  </para>

  <procedure xml:id="pro-ha-geo-quick-ha-cluster-geo-init-arbitrator">
   <title>Setting Up the Arbitrator with <command>ha-cluster-geo-init-arbitrator</command></title>
   <step>
    <para>
     Log in to the machine you want to use as arbitrator.
    </para>
   </step>
   <step>
    <para>
     Run the following command. For example:
    </para>
<screen><prompt role="root">root # </prompt><command>ha-cluster-geo-init-arbitrator</command> --cluster-node<co xml:id="co-geo-init-arbitrator-cluster-node"/> 192.168.201.100</screen>
    <calloutlist>
     <callout arearefs="co-geo-init-arbitrator-cluster-node">
      <para>
       Specifies where to copy the booth configuration from. Use the IP address
       or host name of a node in an already configured Geo
       cluster site. Alternatively, use the virtual IP address of an already
       existing cluster site (like in this example).
      </para>
     </callout>
    </calloutlist>
   </step>
  </procedure>

  <para>
   The <command>ha-cluster-geo-init-arbitrator</command> script copies the
   booth configuration from
   <xref linkend="co-geo-init-arbitrator-cluster-node"/>, see
   <xref linkend="ex-geo-init-booth-conf" xrefstyle="select:label"/>. It also
   enables and starts the booth service on the arbitrator. Thus, the arbitrator
   is ready to communicate with the booth instances on the cluster sites when
   the booth services are running there too.
  </para>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-monitor">
  <title>Monitoring the Cluster Sites</title>

  <para>
   To view both cluster sites with the resources and the ticket that you have
   created during the bootstrapping process, use Hawk2. The Hawk2 Web
   interface allows you to monitor and manage multiple (unrelated) clusters and
   Geo clusters.
  </para>

  <itemizedlist>
   <title>Prerequisites</title>
   <listitem>
    <para>
     All clusters to be monitored from Hawk2's <guimenu>Dashboard</guimenu>
     must be running <phrase role="productname"><phrase os="sles">SUSE Linux Enterprise High Availability Extension</phrase></phrase> <phrase role="productnumber"><phrase os="sles">15 SP2</phrase></phrase>.
    </para>
   </listitem>
   <listitem>
    <para>
     If you did not replace the self-signed certificate for Hawk2 on every
     cluster node with your own certificate (or a certificate signed by an
     official Certificate Authority) yet, do the following: Log in to Hawk2 on
     <emphasis>every</emphasis> node in <emphasis>every</emphasis> cluster
     at least once. Verify the certificate (or add an exception in the
     browser to bypass the warning). Otherwise Hawk2 cannot connect to the
     cluster.
    </para>
   </listitem>
  </itemizedlist>

  <procedure xml:id="pro-ha-geo-quick-hawk2-dashboard">
   <title>Using the Hawk2 Dashboard</title>
   <step>
    <para>
     Start a Web browser and enter the virtual IP of your first cluster site,
     <literal>amsterdam</literal>:
    </para>
<screen>https://192.168.201.100:7630/</screen>
    <para>
     Alternatively, use the IP address or host name of
     <literal>alice</literal> or <literal>bob</literal>. If you have set
     up both nodes with the bootstrap scripts, the <literal>hawk</literal>
     service should run on both nodes.
    </para>
   </step>
   <step>
    <para>
     Log in to the Hawk2 Web interface.
    </para>
   </step>
   <step>
    <para>
     From the left navigation bar, select <guimenu>Dashboard</guimenu>.
    </para>
    <para>
     Hawk2 shows an overview of the resources and nodes on the current
     cluster site. In addition, it shows any <guimenu>Tickets</guimenu> that
     have been configured for the Geo cluster. If you need information about
     the icons used in this view, click <guimenu>Legend</guimenu>.
    </para>
    <figure>
     <title>Hawk2 Dashboard with One Cluster Site (<literal>amsterdam</literal>)</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk2-dashboard-site1.png" width="100%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="hawk2-dashboard-site1.png" width="95%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
   <step>
    <para>
     To add a dashboard for the second cluster site, click <guimenu>Add
     Cluster</guimenu>.
    </para>
    <substeps>
     <step>
      <para>
       Enter the <guimenu>Cluster name</guimenu> with which to identify the
       cluster in the <guimenu>Dashboard</guimenu>. In this case,
       <literal>berlin</literal>.
      </para>
     </step>
     <step>
      <para>
       Enter the fully qualified host name of one of the cluster nodes (in this
       case, <literal>charlie</literal> or <literal>doro</literal>).
      </para>
     </step>
     <step>
      <para>
       Click <guimenu>Add</guimenu>. Hawk2 will display a second tab for the
       newly added cluster site with an overview of its nodes and resources.
      </para>
      <figure>
       <title>Hawk2 Dashboard with Both Cluster Sites</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="hawk2-dashboard-two-sites.png" width="100%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="hawk2-dashboard-two-sites.png" width="95%" format="PNG"/>
        </imageobject>
       </mediaobject>
      </figure>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To view more details for a cluster site or to manage it, switch to the
     site's tab and click the chain icon.
    </para>
    <para>
     Hawk2 opens the <guimenu>Status</guimenu> view for this site in a new
     browser window or tab. From there, you can administer this part of the
     Geo cluster.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-next">
  <title>Next Steps</title>

  <para>
   The Geo clustering bootstrap scripts provide a quick way to set up a basic
   Geo cluster that can be used for testing purposes. However, to move the
   resulting Geo cluster into a functioning Geo cluster that can be used in
   production environments, more steps are required—see
   <xref linkend="vl-ha-geo-quick-next-req"/>.
  </para>

  <variablelist xml:id="vl-ha-geo-quick-next-req">
   <title>Required Steps to Complete the Geo Cluster Setup</title>
   <varlistentry xml:id="vle-ha-geo-quick-booth-service-sites">
    <term>Starting the Booth Services on Cluster Sites</term>
    <listitem>
     <para>
      After the bootstrap process, the arbitrator booth service cannot communicate
      with the booth services on the cluster sites yet, because they are not
      started by default.
     </para>
     <para>
      The booth service for each cluster site is managed by the booth resource
      group <literal>g-booth</literal> (see
      <xref linkend="ex-geo-init-rsc-conf"/>). To start one instance of the
      booth service per site, start the respective booth resource group on each
      cluster site. This enables all booth instances to communicate with each
      other.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Configuring Ticket Dependencies and Ordering Constraints</term>
    <listitem>
     <para>
      To make resources depend on the ticket that you have created during the
      Geo cluster bootstrap process, configure constraints. For each
      constraint, set a <literal>loss-policy</literal> that defines what should
      happen to the respective resources if the ticket is revoked from a cluster
      site.
     </para>
     <para>
      For details, see <xref linkend="cha-ha-geo-rsc"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Initially Granting a Ticket to a Site</term>
    <listitem>
     <para>
      Before booth can manage a certain ticket within the Geo cluster, you
      initially need to <emphasis>grant</emphasis> it to a site manually. You
      can use either the booth client command line tool or Hawk2 to grant a
      ticket.
     </para>
     <para>
      For details, see <xref linkend="cha-ha-geo-manage"/>.</para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The bootstrap scripts create the same booth resources on both cluster sites,
   and the same booth configuration files on all sites, including the
   arbitrator. If you extend the Geo cluster setup (to move to a production
   environment), you will probably fine-tune the booth configuration and
   change the configuration of the booth-related cluster resources. Afterward,
   you need to synchronize the changes to the other sites of your Geo cluster
   to take effect.
  </para>

  <note>
   <title>Synchronizing Changes Across Cluster Sites</title>
   <itemizedlist>
    <listitem>
     <para>
      To synchronize changes in the booth configuration to all cluster sites
      (including the arbitrator), use Csync2. Find more information at <xref linkend="cha-ha-geo-sync"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      The CIB (Cluster Information Database) is not automatically synchronized
      across cluster sites of a Geo cluster. That means any changes in
      resource configuration that are required on all cluster sites need to be
      transferred to the other sites manually. Do so by tagging the
      respective resources, exporting them from the current CIB, and importing
      them to the CIB on the other cluster sites. For details, see
      <xref linkend="sec-ha-geo-rsc-sync-cib"/>.
      </para>
    </listitem>
   </itemizedlist>
  </note>
 </sect1>
 <sect1 xml:id="sec-ha-geo-quick-more">
  <title>For More Information</title>
  <itemizedlist>
   <listitem>
    <para>
     More documentation for this product is available at
     <link xlink:href="https://documentation.suse.com/sle-ha/15-SP2"/>.
     For further configuration and administration tasks, see the comprehensive
     <link xlink:href="https://documentation.suse.com/sle-ha/15-SP2/html/SLE-HA-all/book-sleha-geo.html">
      <citetitle>Geo Clustering Guide</citetitle></link>.
    </para>
   </listitem>
   <listitem>
    <para>
     Find information about data replication across Geo clusters via DRBD in the following
     <link xlink:href="https://documentation.suse.com/sbp/all/html/SBP-DRBD/index.html"><citetitle>SUSE Best Practices</citetitle>
     document</link>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <xi:include href="common_copyright_quick.xml"/>
 <xi:include href="common_legal.xml"/>
</article>
